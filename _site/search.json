[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Does Height Matter When Running for President?",
    "section": "",
    "text": "Height is supposed to confer all sorts of advantages in life. Taller people make more money, have an easier time finding romantic partners, and can reach things off the highest shelves without using a step stool. But does height matter when it comes to politics? The topic has been the subject of extensive debate‚Äîso much so that a Wikipedia page was written to provide information on the heights of US presidential candidates. In this post I analyze this debate quantitatively using R and Bayesian regression methods. My results conclusively show that height probably doesn‚Äôt matter much when it comes to winning the presidency."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bertrand Wilden",
    "section": "",
    "text": "Hello! I am a PhD data scientist specializing in Bayesian modeling methods. My research involves building statistical models to measure latent variables such as political ideology, and developing methods for incorporating the uncertainty from these measurements into downstream analysis. This blog is for showcasing some of my research and other odd quantitative projects I am working on.\nWhen I‚Äôm waiting for my MCMC chains to finish I like to rock climb, play board games, and go backpacking."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "When waiting for my MCMC chains to finish I like to rock climb, play board games, and go backpacking."
  },
  {
    "objectID": "posts/post-with-code/index.html#getting-the-data",
    "href": "posts/post-with-code/index.html#getting-the-data",
    "title": "Does Height Matter When Running for President?",
    "section": "Getting the data",
    "text": "Getting the data\nThe first thing to do is gather the data on presidential candidate heights. The package rvest is a great way to scrape the Wikipedia page above. It‚Äôs pretty easy to get data off Wikipedia because the HTML is relatively simple. But tables of data on Wikipedia need a bit of cleaning before they can be used for any statistical analysis. You have to remove things like citation markers, as well as fix column names and make sure columns containing numbers are actually numeric types.\n\nurl <- \"https://en.wikipedia.org/wiki/Heights_of_presidents_and_presidential_candidates_of_the_United_States\"\n\nheight_table <- url |> \n  # Parse the raw html\n  read_html() |> \n  # Pull out the table elements\n  html_elements(\"table\") |> \n  purrr::pluck(5) |> \n  # Turn the candidate height table into a tibble\n  html_table()\n\nheights <- height_table |> \n  # Assign names to all columns to fix duplicate originals\n  `colnames<-`(c(\"election\", \"winner\", \"winner_height_in\", \"winner_height_cm\",\n               \"opponent\", \"opponent_height_in\", \"opponent_height_cm\",\n               \"difference_in\", \"difference_cm\")) |> \n  # Removing problematic elections\n  filter(!election %in% c(\"1912\", \"1860\", \"1856\", \"1836\", \"1824\"),\n         opponent_height_cm != \"\") |> \n  # Cleaning up the citation markers and fixing column types\n  mutate(across(everything(),\n                ~ str_remove_all(., \"\\\\[.*\\\\]\")),\n         across(contains(\"_cm\"), \n                ~ str_remove_all(.x, \"\\\\D\") |> \n                  as.numeric()),\n         # Making a few new variable for the analysis\n         winner_difference_cm = winner_height_cm - opponent_height_cm,\n         winner_taller = if_else(winner_difference_cm > 0, 1, 0))\n\nIn the process of cleaning the presidential candidate height data I decided to remove all elections in which more than two candidates ran (1824, 1836, 1856, 1860, 1912), all elections in which a candidate‚Äôs height was missing from Wikipedia (1816: Rufus King, 1868: Horatio Seymour), and all uncontested elections (1788 and 1792: George Washington, 1820 James Monroe). No information regarding a height advantage can be gleaned from the latter two categories (unless it was Washington‚Äôs large stature that helped dissuade any potential challengers) so their exclusion should be uncontroversial. The removal of multi-candidate elections, however, was a choice I made in order to simplify the analysis. The role of height in a multi-candidate election is less straightforward than in a two-candidate election. Should we suppose voters simply gravitate towards the tallest candidate running? Or are they making height comparisons between all three candidates at once? Because political science lacks a good theory to support any of these explanations I dropped multi-candidate elections and moved on.\nAfter these cleaning steps I made a new variable called winner_taller which simply denotes whether that taller candidate won the particular election, 1 or lost, 0. Using mean(heights$winner_taller) we see that the proportion of elections won by the taller candidate is 0.551. The taller candidate wins more on average! Skeptical readers will object that the sample size is too low for this result to be conclusive. ‚ÄúWhat is the standard error of the proportion!‚Äù they will say, ‚ÄúI want to see a p-value!‚Äù These are valid critiques, but as a fervent Bayesian I refuse to calculate any p-values. Let‚Äôs move on to some further analysis."
  },
  {
    "objectID": "posts/post-with-code/index.html#presidential-candidates-compared-to-the-general-population",
    "href": "posts/post-with-code/index.html#presidential-candidates-compared-to-the-general-population",
    "title": "Does Height Matter When Running for President?",
    "section": "Presidential candidates compared to the general population",
    "text": "Presidential candidates compared to the general population\nThe original candidate height data set was at the election-level, meaning that every row represented a presidential election year. In order to look at candidate heights individually, I transformed the data into ‚Äúlong‚Äù format such that each row represents a single candidate. With the data at the candidate-level, we can now investigate how the heights of presidential candidates compare to the overall population.\n\nheights_long <- heights |> \n  pivot_longer(cols = c(\"winner\", \"opponent\"),\n               values_to = \"candidate\",\n               names_to = \"status\") |> \n  # Creating a single variable for candidate height\n  mutate(height_cm = case_when(status == \"winner\" ~ winner_height_cm,\n                               status == \"opponent\" ~ opponent_height_cm))\n\nThe graph below shows the distribution of candidate heights compared to the US adult male population. The variable ‚Äúheight‚Äù is often used to illustrate a Normal distribution in action. But technically, the Normal distribution does not accurately reflect height unless we first narrow the population down. Children and adults do not share the same height distribution, and neither do different genders. Each country, or region of the globe, likely also has a distinct height distribution. So unless we clearly define which population we‚Äôre talking about, ‚Äúheight‚Äù is best characterized as a mixture of normal distributions. Since almost all US presidential candidates have been adult men, however, I overlaid only the distribution for US adult males (mean 178 cm, standard deviation 8 cm).\n\nheights_long |>\n  select(height_cm, candidate) |> \n  distinct() |> \n  ggplot() +\n  stat_function(geom = \"textpath\", vjust = 0, hjust = .2,\n                label = \"US Male Population\",\n                fun = function(x) dnorm(x, mean = 178, sd = 8) * 20) +\n  geom_dots(aes(x = height_cm,\n                fill = candidate == \"Hillary Clinton\",\n                group = NA),\n            size = .1) +\n  scale_fill_manual(values = met.brewer(\"Lakota\", 2)) +\n  xlim(150, 200) +\n  labs(x = \"Height in cm\", y = \"\",\n       title = \"Heights of US Presidential Candidates\\nCompared to US Male Population\") +\n  theme(legend.position = \"none\",\n        axis.line.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\nHillary Clinton (represented by the yellow dot in the candidate distribution) should not be compared to the average US male in terms of height‚Äîbut interestingly, isn‚Äôt the shortest candidate in US history. That honor goes to James Madison at 163 cm (5‚Äô 4‚Äù). The graph shows that presidential candidates roughly align with overall male population heights. Perhaps candidates are slightly taller than the average US male, but the difference appears small."
  },
  {
    "objectID": "posts/post-with-code/index.html#how-much-does-height-contribute-to-winning",
    "href": "posts/post-with-code/index.html#how-much-does-height-contribute-to-winning",
    "title": "Does Height Matter When Running for President?",
    "section": "How much does height contribute to winning?",
    "text": "How much does height contribute to winning?\n\nheights_long <- heights_long |> \n  mutate(winner = if_else(status == \"winner\", 1, 0))\n\n\nheight_model <- brm(\n  winner ~ height_cm,\n  prior = prior(normal(0, 2), class = \"b\") +\n          prior(normal(0, 2), class = \"Intercept\"),\n  family = bernoulli(link = \"logit\"),\n  data = heights_long,\n  seed = 111,\n  refresh = 0,\n  iter = 12000,\n  backend = \"cmdstanr\"\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.3 seconds.\n\n\n\nprediction_grid <- with(heights_long, \n  data.frame(height_cm = seq(min(height_cm), max(height_cm), length.out = 100))\n)\n\n\nprediction_grid |> \n  add_epred_draws(height_model, ndraws = 100) |> \n  group_by(height_cm) |> \n  summarise(.median = median(.epred),\n            .sd = sd(.epred)) |> \n  mutate(log_odds = dist_normal(.median, .sd),\n         p_winner = dist_transformed(log_odds, plogis, qlogis)) |> \n  ggplot(aes(x = height_cm)) +\n  geom_dots(\n    aes(y = winner, side = ifelse(winner == 1, \"bottom\", \"top\")),\n    scale = 0.4,\n    fill = \"#931e18\",\n    size = .1,\n    data = heights_long\n  ) +\n  stat_lineribbon(\n    aes(ydist = p_winner), alpha = .25, fill = \"#931e18\", size = .5\n  )\n\n\n\n\n\nprediction_grid <- with(heights_long, \n  data.frame(height_cm = 206)\n)\n\nprediction_grid |> \n  add_epred_draws(height_model, ndraws = 12000) |> \n  mutate(p_winner = 1 / (1 + exp(-.epred))) |> \n  ggplot(aes(x = p_winner)) +\n  stat_slabinterval(fill = \"#04a3bd\", trim = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html#how-much-does-height-contribute-to-winning-the-presidency",
    "href": "posts/post-with-code/index.html#how-much-does-height-contribute-to-winning-the-presidency",
    "title": "Does Height Matter When Running for President?",
    "section": "How much does height contribute to winning the presidency?",
    "text": "How much does height contribute to winning the presidency?\nOkay, so we discovered that the taller candidate wins slightly more often on average, but how does raw height affect a candidate‚Äôs chances of becoming president? To answer this question, we need to add a new dummy variable to our candidate-level data set indicating whether they won or lost.\n\nheights_long <- heights_long |> \n  mutate(winner = if_else(status == \"winner\", 1, 0))\n\nThen I fit the following Bayesian logistic regression model to the data:\n\\[\\begin{equation*}\n\\begin{aligned}\n\\text{Winner}_i &\\sim \\text{Bernoulli}(p) \\\\\np &= \\text{logit}^{-1}(\\alpha + \\beta \\ \\text{Height}_i) \\\\\n\\alpha &\\sim \\text{Normal}(0, 2) \\\\\n\\beta &\\sim \\text{Normal}(0, 2)\n\\end{aligned}\n\\end{equation*}\\]\nThere‚Äôs nothing too fancy going on in this model‚Äîjust a standard logistic regression with a binary outcome (winning the presidency or not winning the presidency). The Normal(0, 2) priors on the intercept and slope coefficients are weakly informative, meaning they are wide enough to let the data inform our results, but narrow enough to be skeptical of extreme values. Given background knowledge of height in presidential campaigns, it‚Äôs unlikely it has a big effect on the outcome.\nThe code below fits the model using the brms package in R. Because the data only contain 96 candidate observations, the MCMC chains converge extremely quickly. Only 1.2 seconds for 12,000 iterations! Good practice when working with Bayesian models dictates that we look into the diagnostic measures (R-hat, effective sample size, number of divergent transitions, etc) of our fitted model. But since this model is very simple, I hope you will trust me that the fitting process worked reliably well.\n\nheight_model <- brm(\n  winner ~ 1 + height_cm,\n  prior = prior(normal(0, 2), class = \"b\") +\n          prior(normal(0, 2), class = \"Intercept\"),\n  family = bernoulli(link = \"logit\"),\n  data = heights_long,\n  seed = 111,\n  refresh = 0,\n  iter = 12000,\n  backend = \"cmdstanr\"\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.3 seconds.\n\n\nAttempting to directly interpret coefficient values from logit models is rarely a good idea. Instead we can graph the results and compare the predicted probabilities of the outcome variable (winning the presidency) against a range of input variable values (candidate height in cm). This is what the (logit dotplot)[https://www.barelysignificant.com/post/glm/] below shows. The dots on the top and bottom of the graph represent candidates that either won or lost, and the line between them shows what our model predicts the winning probability to be at each height value on the x-axis. the weakly upward slope on this prediction line tells us that there is barely any benefit to being an extra cm taller when it comes to winning a presidential election.\n\n# Generate a set of values across the range of the height data\nprediction_grid <- with(heights_long, \n  data.frame(height_cm = seq(min(height_cm), max(height_cm), length.out = 100))\n)\n\nprediction_grid |> \n  # Generate posterior draws\n  add_epred_draws(height_model, ndraws = 100) |> \n  # Collapse down to the height level\n  group_by(height_cm) |> \n  summarise(.median = median(.epred),\n            .sd = sd(.epred)) |> \n  # Convert log odds into predicted probabilities\n  mutate(log_odds = dist_normal(.median, .sd),\n         p_winner = dist_transformed(log_odds, plogis, qlogis)) |> \n  ggplot(aes(x = height_cm)) +\n  geom_dots(\n    aes(y = winner, side = ifelse(winner == 1, \"bottom\", \"top\")),\n    scale = 0.4,\n    fill = \"#931e18\",\n    size = .1,\n    data = heights_long) +\n  stat_lineribbon(\n    aes(ydist = p_winner), alpha = .25, fill = \"#931e18\", size = .5) +\n  labs(title = \"Predicted Probability of Winning the Presidency\\nBased on Candidate Height\",\n       x = \"Height cm\",\n       y = \"Pr(Winning)\")\n\n\n\n\nApparently height has little effect on the probability that a candidate wins the presidency. But what if the candidate was extremely tall? It is now time to make a confession. The true reason I started this project was for selfish reasons. As someone who is 206 cm tall (6‚Äô 9‚Äù), I wanted to know what my chances were of becoming president based only on my height. Plugging my 206 cm into the logistic regression model produces the posterior probability distribution shown in the graph below. While there is considerable uncertainty due to the small sample size of candidates, the model says I have between a 60 and 70% chance to win. Amazing!\n\nprediction_grid <- with(heights_long, \n  data.frame(height_cm = 206)\n)\n\nprediction_grid |> \n  add_epred_draws(height_model, ndraws = 12000) |> \n  mutate(p_winner = 1 / (1 + exp(-.epred))) |> \n  ggplot(aes(x = p_winner)) +\n  stat_slabinterval(fill = \"#04a3bd\", trim = FALSE) +\n  labs(title = \"Posterior Probability of Winning\\nfor Someone Very Tall\",\n       x = \"Pr(Winning | Height = 206 cm)\",\n       y = \"\") +\n  theme(axis.line.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\nAs we all know, numbers don‚Äôt lie. So keep an eye out for the Bert‚Äì2024 campaign coming soon."
  },
  {
    "objectID": "posts/post-with-code/index.html#session-info",
    "href": "posts/post-with-code/index.html#session-info",
    "title": "Does Height Matter When Running for President?",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.2.9000 geomtextpath_0.1.0   distributional_0.3.0\n [4] brms_2.17.4          Rcpp_1.0.8.3         ggdist_3.0.99.9000  \n [7] MetBrewer_0.2.0      rvest_1.0.2          forcats_0.5.1       \n[10] stringr_1.4.0        dplyr_1.0.9          purrr_0.3.4         \n[13] readr_2.1.2          tidyr_1.2.0          tibble_3.1.7        \n[16] ggplot2_3.3.6        tidyverse_1.3.1     \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.3.1         backports_1.4.1      systemfonts_1.0.3   \n  [4] selectr_0.4-2        plyr_1.8.7           igraph_1.3.1        \n  [7] svUnit_1.0.6         splines_4.1.1        crosstalk_1.2.0     \n [10] rstantools_2.2.0     inline_0.3.19        digest_0.6.29       \n [13] htmltools_0.5.2      fansi_1.0.3          magrittr_2.0.3      \n [16] checkmate_2.1.0      tzdb_0.2.0           modelr_0.1.8        \n [19] RcppParallel_5.1.5   matrixStats_0.62.0   xts_0.12.1          \n [22] prettyunits_1.1.1    colorspace_2.0-3     textshaping_0.3.6   \n [25] haven_2.4.3          xfun_0.31            callr_3.7.0         \n [28] crayon_1.5.1         jsonlite_1.8.0       lme4_1.1-27.1       \n [31] zoo_1.8-10           glue_1.6.2           gtable_0.3.0        \n [34] emmeans_1.7.2        V8_4.2.0             pkgbuild_1.3.1      \n [37] rstan_2.26.11        abind_1.4-5          scales_1.2.0        \n [40] mvtnorm_1.1-3        DBI_1.1.1            miniUI_0.1.1.1      \n [43] viridisLite_0.4.0    xtable_1.8-4         stats4_4.1.1        \n [46] StanHeaders_2.26.11  DT_0.23              htmlwidgets_1.5.4   \n [49] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  \n [52] posterior_1.2.1      ellipsis_0.3.2       pkgconfig_2.0.3     \n [55] loo_2.5.1            farver_2.1.0         dbplyr_2.1.1        \n [58] utf8_1.2.2           labeling_0.4.2       tidyselect_1.1.2    \n [61] rlang_1.0.2          reshape2_1.4.4       later_1.3.0         \n [64] munsell_0.5.0        cellranger_1.1.0     tools_4.1.1         \n [67] cli_3.3.0            generics_0.1.2       broom_0.8.0         \n [70] ggridges_0.5.3       evaluate_0.15        fastmap_1.1.0       \n [73] yaml_2.3.5           processx_3.5.3       knitr_1.39          \n [76] fs_1.5.2             nlme_3.1-152         mime_0.12           \n [79] projpred_2.0.2       xml2_1.3.2           compiler_4.1.1      \n [82] bayesplot_1.9.0      shinythemes_1.2.0    rstudioapi_0.13     \n [85] curl_4.3.2           gamm4_0.2-6          reprex_2.0.1        \n [88] stringi_1.7.6        ps_1.7.0             Brobdingnag_1.2-7   \n [91] lattice_0.20-44      Matrix_1.3-4         nloptr_1.2.2.2      \n [94] markdown_1.1         shinyjs_2.1.0        tensorA_0.36.2      \n [97] vctrs_0.4.1          pillar_1.7.0         lifecycle_1.0.1     \n[100] bridgesampling_1.1-2 estimability_1.3     data.table_1.14.2   \n[103] httpuv_1.6.5         R6_2.5.1             promises_1.2.0.1    \n[106] gridExtra_2.3        codetools_0.2-18     boot_1.3-28         \n[109] colourpicker_1.1.1   MASS_7.3-54          gtools_3.9.2.1      \n[112] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     \n[115] mgcv_1.8-36          parallel_4.1.1       hms_1.1.1           \n[118] grid_4.1.1           coda_0.19-4          minqa_1.2.4         \n[121] cmdstanr_0.4.0       rmarkdown_2.14       shiny_1.7.1         \n[124] lubridate_1.7.10     base64enc_0.1-3      dygraphs_1.1.1.6"
  },
  {
    "objectID": "posts/president-height/index.html",
    "href": "posts/president-height/index.html",
    "title": "Does Height Matter When Running for President?",
    "section": "",
    "text": "Height is supposed to confer all sorts of advantages in life. Taller people make more money, have an easier time finding romantic partners, and can reach things off the highest shelves without using a step stool. But does height matter when it comes to politics? The topic has been the subject of extensive debate‚Äîso much so that a Wikipedia page was written to provide information on the heights of US presidential candidates. In this post I analyze this debate quantitatively using R and Bayesian regression methods. My results conclusively show that height probably doesn‚Äôt matter much when it comes to winning the presidency.\n\n# Loading in the packages used\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(MetBrewer)\nlibrary(ggdist)\nlibrary(brms)\nlibrary(distributional)\nlibrary(geomtextpath)\nlibrary(tidybayes)\n\n# Global plotting theme for ggplot\ntheme_set(theme_ggdist())\n\n# Set global rounding options\noptions(scipen = 1, \n        digits = 3)"
  },
  {
    "objectID": "posts/president-height/index.html#getting-the-data",
    "href": "posts/president-height/index.html#getting-the-data",
    "title": "Does Height Matter When Running for President?",
    "section": "Getting the data",
    "text": "Getting the data\nThe first thing to do is gather the data on presidential candidate heights. The package rvest is a great way to scrape the Wikipedia page above. It‚Äôs pretty easy to get data off Wikipedia because the HTML is relatively simple. But tables of data on Wikipedia need a bit of cleaning before they can be used for any statistical analysis. You have to remove things like citation markers, as well as fix column names and make sure columns containing numbers are actually numeric types.\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Heights_of_presidents_and_presidential_candidates_of_the_United_States\"\n\nheight_table &lt;- url |&gt; \n  # Parse the raw html\n  read_html() |&gt; \n  # Pull out the table elements\n  html_elements(\"table\") |&gt; \n  purrr::pluck(5) |&gt; \n  # Turn the candidate height table into a tibble\n  html_table()\n\nheights &lt;- height_table |&gt; \n  # Assign names to all columns to fix duplicate originals\n  `colnames&lt;-`(c(\"election\", \"winner\", \"winner_height_in\", \"winner_height_cm\",\n               \"opponent\", \"opponent_height_in\", \"opponent_height_cm\",\n               \"difference_in\", \"difference_cm\")) |&gt; \n  # Removing problematic elections\n  filter(!election %in% c(\"1912\", \"1860\", \"1856\", \"1836\", \"1824\"),\n         opponent_height_cm != \"\") |&gt; \n  # Cleaning up the citation markers and fixing column types\n  mutate(across(everything(),\n                ~ str_remove_all(., \"\\\\[.*\\\\]\")),\n         across(contains(\"_cm\"), \n                ~ str_remove_all(.x, \"\\\\D\") |&gt; \n                  as.numeric()),\n         # Making a few new variable for the analysis\n         winner_difference_cm = winner_height_cm - opponent_height_cm,\n         winner_taller = if_else(winner_difference_cm &gt; 0, 1, 0))\n\nIn the process of cleaning the presidential candidate height data I decided to remove all elections in which more than two candidates ran (1824, 1836, 1856, 1860, 1912), all elections in which a candidate‚Äôs height was missing from Wikipedia (1816: Rufus King, 1868: Horatio Seymour), and all uncontested elections (1788 and 1792: George Washington, 1820 James Monroe). No information regarding a height advantage can be gleaned from the latter two categories (unless it was Washington‚Äôs large stature that helped dissuade any potential challengers) so their exclusion should be uncontroversial. The removal of multi-candidate elections, however, was a choice I made in order to simplify the analysis. The role of height in a multi-candidate election is less straightforward than in a two-candidate election. Should we suppose voters simply gravitate towards the tallest candidate running? Or are they making height comparisons between all three candidates at once? Because political science lacks a good theory to support any of these explanations I dropped multi-candidate elections and moved on.\nAfter these cleaning steps I made a new variable called winner_taller which simply denotes whether that taller candidate won the particular election, 1 or lost, 0. Using mean(heights$winner_taller) we see that the proportion of elections won by the taller candidate is 0.551. The taller candidate wins more on average! Skeptical readers will object that the sample size is too low for this result to be conclusive. ‚ÄúWhat is the standard error of the proportion!‚Äù they will say, ‚ÄúI want to see a p-value!‚Äù These are valid critiques, but as a fervent Bayesian I refuse to calculate any p-values. Let‚Äôs move on to some further analysis."
  },
  {
    "objectID": "posts/president-height/index.html#presidential-candidates-compared-to-the-general-population",
    "href": "posts/president-height/index.html#presidential-candidates-compared-to-the-general-population",
    "title": "Does Height Matter When Running for President?",
    "section": "Presidential candidates compared to the general population",
    "text": "Presidential candidates compared to the general population\nThe original candidate height data set was at the election-level, meaning that every row represented a presidential election year. In order to look at candidate heights individually, I transformed the data into ‚Äúlong‚Äù format such that each row represents a single candidate. With the data at the candidate-level, we can now investigate how the heights of presidential candidates compare to the overall population.\n\nheights_long &lt;- heights |&gt; \n  pivot_longer(cols = c(\"winner\", \"opponent\"),\n               values_to = \"candidate\",\n               names_to = \"status\") |&gt; \n  # Creating a single variable for candidate height\n  mutate(height_cm = case_when(status == \"winner\" ~ winner_height_cm,\n                               status == \"opponent\" ~ opponent_height_cm))\n\nThe graph below shows the distribution of candidate heights compared to the US adult male population. The variable ‚Äúheight‚Äù is often used to illustrate a Normal distribution in action. But technically, the Normal distribution does not accurately reflect height unless we first narrow the population down. Children and adults do not share the same height distribution, and neither do different genders. Each country, or region of the globe, likely also has a distinct height distribution. So unless we clearly define which population we‚Äôre talking about, ‚Äúheight‚Äù is best characterized as a mixture of normal distributions. Since almost all US presidential candidates have been adult men, however, I overlaid only the distribution for US adult males (mean 178 cm, standard deviation 8 cm).\n\nheights_long |&gt;\n  select(height_cm, candidate) |&gt; \n  distinct() |&gt; \n  ggplot() +\n  stat_function(geom = \"textpath\", vjust = 0, hjust = .2,\n                label = \"US Male Population\",\n                fun = function(x) dnorm(x, mean = 178, sd = 8) * 20) +\n  geom_dots(aes(x = height_cm,\n                fill = candidate == \"Hillary Clinton\",\n                group = NA),\n            size = .1) +\n  scale_fill_manual(values = met.brewer(\"Lakota\", 2)) +\n  xlim(150, 200) +\n  labs(x = \"Height in cm\", y = \"\",\n       title = \"Heights of US Presidential Candidates\\nCompared to US Male Population\") +\n  theme(legend.position = \"none\",\n        axis.line.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\n\n\n\n\nHillary Clinton (represented by the yellow dot in the candidate distribution) should not be compared to the average US male in terms of height‚Äîbut interestingly, isn‚Äôt the shortest candidate in US history. That honor goes to James Madison at 163 cm (5‚Äô 4‚Äù). The graph shows that presidential candidates roughly align with overall male population heights. Perhaps candidates are slightly taller than the average US male, but the difference appears small."
  },
  {
    "objectID": "posts/president-height/index.html#how-much-does-height-contribute-to-winning-the-presidency",
    "href": "posts/president-height/index.html#how-much-does-height-contribute-to-winning-the-presidency",
    "title": "Does Height Matter When Running for President?",
    "section": "How much does height contribute to winning the presidency?",
    "text": "How much does height contribute to winning the presidency?\nOkay, so we discovered that the taller candidate wins slightly more often on average, but how does raw height affect a candidate‚Äôs chances of becoming president? To answer this question, we need to add a new dummy variable to our candidate-level data set indicating whether they won or lost.\n\nheights_long &lt;- heights_long |&gt; \n  mutate(winner = if_else(status == \"winner\", 1, 0))\n\nThen I fit the following Bayesian logistic regression model to the data:\n\\[\\begin{equation*}\n\\begin{aligned}\n\\text{Winner}_i &\\sim \\text{Bernoulli}(p) \\\\\np &= \\text{logit}^{-1}(\\alpha + \\beta \\ \\text{Height}_i) \\\\\n\\alpha &\\sim \\text{Normal}(0, 2) \\\\\n\\beta &\\sim \\text{Normal}(0, 2)\n\\end{aligned}\n\\end{equation*}\\]\nThere‚Äôs nothing too fancy going on in this model‚Äîjust a standard logistic regression with a binary outcome (winning the presidency or not winning the presidency). The Normal(0, 2) priors on the intercept and slope coefficients are weakly informative, meaning they are wide enough to let the data inform our results, but narrow enough to be skeptical of extreme values. Given background knowledge of height in presidential campaigns, it‚Äôs unlikely it has a big effect on the outcome.\nThe code below fits the model using the brms package in R. Because the data only contain 96 candidate observations, the MCMC chains converge extremely quickly. Only 1.2 seconds for 12,000 iterations! Good practice when working with Bayesian models dictates that we look into the diagnostic measures (R-hat, effective sample size, number of divergent transitions, etc) of our fitted model. But since this model is very simple, I hope you will trust me that the fitting process worked reliably well.\n\nheight_model &lt;- brm(\n  winner ~ 1 + height_cm,\n  prior = prior(normal(0, 2), class = \"b\") +\n          prior(normal(0, 2), class = \"Intercept\"),\n  family = bernoulli(link = \"logit\"),\n  data = heights_long,\n  seed = 111,\n  refresh = 0,\n  iter = 12000,\n  backend = \"cmdstanr\"\n)\n\nStart sampling\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.3 seconds.\n\n\nAttempting to directly interpret coefficient values from logit models is rarely a good idea. Instead we can graph the results and compare the predicted probabilities of the outcome variable (winning the presidency) against a range of input variable values (candidate height in cm). This is what the (logit dotplot)[https://www.barelysignificant.com/post/glm/] below shows. The dots on the top and bottom of the graph represent candidates that either won or lost, and the line between them shows what our model predicts the winning probability to be at each height value on the x-axis. the weakly upward slope on this prediction line tells us that there is barely any benefit to being an extra cm taller when it comes to winning a presidential election.\n\n# Generate a set of values across the range of the height data\nprediction_grid &lt;- with(heights_long, \n  data.frame(height_cm = seq(min(height_cm), max(height_cm), length.out = 100))\n)\n\nprediction_grid |&gt; \n  # Generate posterior draws\n  add_epred_draws(height_model, ndraws = 100) |&gt; \n  # Collapse down to the height level\n  group_by(height_cm) |&gt; \n  summarise(.median = median(.epred),\n            .sd = sd(.epred)) |&gt; \n  # Convert log odds into predicted probabilities\n  mutate(log_odds = dist_normal(.median, .sd),\n         p_winner = dist_transformed(log_odds, plogis, qlogis)) |&gt; \n  ggplot(aes(x = height_cm)) +\n  geom_dots(\n    aes(y = winner, side = ifelse(winner == 1, \"bottom\", \"top\")),\n    scale = 0.4,\n    fill = \"#931e18\",\n    size = .1,\n    data = heights_long) +\n  stat_lineribbon(\n    aes(ydist = p_winner), alpha = .25, fill = \"#931e18\", size = .5) +\n  labs(title = \"Predicted Probability of Winning the Presidency\\nBased on Candidate Height\",\n       x = \"Height cm\",\n       y = \"Pr(Winning)\")\n\n\n\n\n\n\n\n\nApparently height has little effect on the probability that a candidate wins the presidency. But what if the candidate was extremely tall? It is now time to make a confession. The true reason I started this project was for selfish reasons. As someone who is 206 cm tall (6‚Äô 9‚Äù), I wanted to know what my chances were of becoming president based only on my height. Plugging my 206 cm into the logistic regression model produces the posterior probability distribution shown in the graph below. While there is considerable uncertainty due to the small sample size of candidates, the model says I have between a 60 and 70% chance to win. Amazing!\n\nprediction_grid &lt;- with(heights_long, \n  data.frame(height_cm = 206)\n)\n\nprediction_grid |&gt; \n  add_epred_draws(height_model, ndraws = 12000) |&gt; \n  mutate(p_winner = 1 / (1 + exp(-.epred))) |&gt; \n  ggplot(aes(x = p_winner)) +\n  stat_slabinterval(fill = \"#04a3bd\", trim = FALSE) +\n  labs(title = \"Posterior Probability of Winning\\nfor Someone Very Tall\",\n       x = \"Pr(Winning | Height = 206 cm)\",\n       y = \"\") +\n  theme(axis.line.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\n\n\n\n\nAs we all know, numbers don‚Äôt lie. So keep an eye out for the Bert‚Äì2024 campaign coming soon."
  },
  {
    "objectID": "posts/president-height/index.html#session-info",
    "href": "posts/president-height/index.html#session-info",
    "title": "Does Height Matter When Running for President?",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.2.9000 geomtextpath_0.1.0   distributional_0.3.0\n [4] brms_2.17.4          Rcpp_1.0.8.3         ggdist_3.0.99.9000  \n [7] MetBrewer_0.2.0      rvest_1.0.2          forcats_0.5.1       \n[10] stringr_1.4.0        dplyr_1.0.9          purrr_0.3.4         \n[13] readr_2.1.2          tidyr_1.2.0          tibble_3.1.7        \n[16] ggplot2_3.3.6        tidyverse_1.3.1     \n\nloaded via a namespace (and not attached):\n  [1] readxl_1.3.1         backports_1.4.1      systemfonts_1.0.3   \n  [4] selectr_0.4-2        plyr_1.8.7           igraph_1.3.1        \n  [7] svUnit_1.0.6         splines_4.1.1        crosstalk_1.2.0     \n [10] rstantools_2.2.0     inline_0.3.19        digest_0.6.29       \n [13] htmltools_0.5.2      fansi_1.0.3          magrittr_2.0.3      \n [16] checkmate_2.1.0      tzdb_0.2.0           modelr_0.1.8        \n [19] RcppParallel_5.1.5   matrixStats_0.62.0   xts_0.12.1          \n [22] prettyunits_1.1.1    colorspace_2.0-3     textshaping_0.3.6   \n [25] haven_2.4.3          xfun_0.31            callr_3.7.0         \n [28] crayon_1.5.1         jsonlite_1.8.0       lme4_1.1-27.1       \n [31] zoo_1.8-10           glue_1.6.2           gtable_0.3.0        \n [34] emmeans_1.7.2        V8_4.2.0             pkgbuild_1.3.1      \n [37] rstan_2.26.11        abind_1.4-5          scales_1.2.0        \n [40] mvtnorm_1.1-3        DBI_1.1.1            miniUI_0.1.1.1      \n [43] viridisLite_0.4.0    xtable_1.8-4         stats4_4.1.1        \n [46] StanHeaders_2.26.11  DT_0.23              htmlwidgets_1.5.4   \n [49] httr_1.4.2           threejs_0.3.3        arrayhelpers_1.1-0  \n [52] posterior_1.2.1      ellipsis_0.3.2       pkgconfig_2.0.3     \n [55] loo_2.5.1            farver_2.1.0         dbplyr_2.1.1        \n [58] utf8_1.2.2           labeling_0.4.2       tidyselect_1.1.2    \n [61] rlang_1.0.2          reshape2_1.4.4       later_1.3.0         \n [64] munsell_0.5.0        cellranger_1.1.0     tools_4.1.1         \n [67] cli_3.3.0            generics_0.1.2       broom_0.8.0         \n [70] ggridges_0.5.3       evaluate_0.15        fastmap_1.1.0       \n [73] yaml_2.3.5           processx_3.5.3       knitr_1.39          \n [76] fs_1.5.2             nlme_3.1-152         mime_0.12           \n [79] projpred_2.0.2       xml2_1.3.2           compiler_4.1.1      \n [82] bayesplot_1.9.0      shinythemes_1.2.0    rstudioapi_0.13     \n [85] curl_4.3.2           gamm4_0.2-6          reprex_2.0.1        \n [88] stringi_1.7.6        ps_1.7.0             Brobdingnag_1.2-7   \n [91] lattice_0.20-44      Matrix_1.3-4         nloptr_1.2.2.2      \n [94] markdown_1.1         shinyjs_2.1.0        tensorA_0.36.2      \n [97] vctrs_0.4.1          pillar_1.7.0         lifecycle_1.0.1     \n[100] bridgesampling_1.1-2 estimability_1.3     data.table_1.14.2   \n[103] httpuv_1.6.5         R6_2.5.1             promises_1.2.0.1    \n[106] gridExtra_2.3        codetools_0.2-18     boot_1.3-28         \n[109] colourpicker_1.1.1   MASS_7.3-54          gtools_3.9.2.1      \n[112] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     \n[115] mgcv_1.8-36          parallel_4.1.1       hms_1.1.1           \n[118] grid_4.1.1           coda_0.19-4          minqa_1.2.4         \n[121] cmdstanr_0.4.0       rmarkdown_2.14       shiny_1.7.1         \n[124] lubridate_1.7.10     base64enc_0.1-3      dygraphs_1.1.1.6"
  },
  {
    "objectID": "posts/probit-probit/index.html",
    "href": "posts/probit-probit/index.html",
    "title": "Probing the Depths of Probit Regression",
    "section": "",
    "text": "# Packages and Global Options\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\nlibrary(unvotes)\nlibrary(ggdist)\n\ntheme_set(theme_minimal())\nWhen do you use a probit model in statistics? When you have some data and want to probe it for answers!\nIt is also a method for modeling a data generating process which results in a binary 0/1 outcome. Maybe you are trying to explain whether someone votes in a particular election, whether a web user clicks on a link, or whether writing statistics blog posts helps its author get a job. Probit models are closely related to their more popular cousin: the logistic, or logit regression. So closely related, in fact, that I can‚Äôt think of any decisive reason why someone would choose one over the other. I like probit regression because I always aspire to maintain a high level of statistical probity in my work. In this post I am going to explain what a probit model is, how to fit one in R, and introduce a brand new nested hurdle probit model."
  },
  {
    "objectID": "posts/probit-probit/index.html#what-is-a-probit-model",
    "href": "posts/probit-probit/index.html#what-is-a-probit-model",
    "title": "Probing the Depths of Probit Regression",
    "section": "What is a Probit Model?",
    "text": "What is a Probit Model?\nAs I mentioned before, we can use a probit model when the outcome of interest is some binary variable. Binary outcomes arise out of what‚Äôs known as a Bernoulli distribution, which we write as:\n\\[\ny_i \\sim \\text{Bernoulli}(p_i)\n\\]\nThe \\(y_i\\) above stands for the observed 1‚Äôs and 0‚Äôs in our data, and the \\(p_i\\) is the probability of a particular \\(y_i\\) equaling 1. For example, the heads (1) and tails (0) we observe from flipping a coin repeatedly would be generated from a Bernoulli distribution with \\(p_i = 0.5\\). We know that \\(p_i = 0.5\\) in the coin flipping example because we have no information that could lead us to expect one outcome over another‚Äîhence a 50/50 probability of getting heads. For more complicated data generating processes, however, we don‚Äôt know \\(p_i\\) in advance. Instead, we use statistical models to rank the relative plausibility of every possible value of \\(p_i\\) based on the data we have collected. The general term for this sort of procedure is called Bayesian updating.\nHow do we go about constructing plausibility rankings for \\(p_i\\)? This is where our probit (probability unit) model comes in. Say we have a number of observed variables, \\(X_{1i}, X_{2i}, X_{3i}\\) whose linear combination we think affects whether \\(y_i = 1\\) or \\(y_i = 0\\) (in other words, \\(p_i\\)). In this case we would like to use an equation that looks like:\n\\[\np_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i}\n\\]\nHere the \\(\\beta\\) terms represent the marginal effect each of the \\(X\\) variables has on \\(p_i\\). But wait! We defined \\(p_i\\) earlier as a probability value‚Äîa real number between 0 and 1. The Bernoulli distribution can‚Äôt give us values for \\(y_i\\) if \\(p_i\\) is not a valid probability. There is nothing in the equation above that enforces the value of the right hand side, \\(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i}\\) to be between 0 and 1 üôÅ. Luckily there is a little guy known as \\(\\Phi\\) who is ready to come to our rescue. We simply wrap our right-hand expression in \\(\\Phi\\)‚Äôs loving embrace and it takes care of transforming the value of these linear predictors to a value on the probability scale of \\(p_i\\):\n\\[\np_i = \\Phi(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i})\n\\]\nWhat is \\(\\Phi\\) and how does it work? It represents the cumulative distribution function (CDF) for a standard Normal distribution, which means that it takes as input any real number and gives you back the probability of getting that value or less out of a standard Normal distribution.1 The graph below shows what this function looks like.\n\nggplot() +\n  stat_function(fun = pnorm, # pnorm is the Normal CDF function in R\n                xlim = c(-4, 4)) +\n  geom_segment(aes(x = 1.5, xend = 1.5, y = 0, yend = pnorm(1.5)),\n                   color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = -4, xend = 1.5, y = pnorm(1.5), yend = pnorm(1.5)),\n                   color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(-4, 4, by = .5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = .1)) +\n  labs(title = \"Standard Normal CDF\",\n       x = \"X\",\n       y = \"Cumulative Probability\")\n\n\n\n\n\n\n\n\nIf we supply \\(\\Phi\\) with an input of 1.5, it gives us an output of 0.93.\nThe standard Normal CDF‚Äôs ability to turn any real number into a probability between 0 and 1 is what allows us to build a model for \\(p_i\\), which then tells us something about the Bernoulli distributions which produced our binary outcome variable \\(y_i\\). As an aside, the difference between probit and logit models comes down to how they transform the linear predictor to the probability scale. Probit uses \\(\\Phi\\) and logit uses the log-odds transformation.2 This changes the scale of the \\(\\beta\\) parameters, and therefore how to interpret their values plus informing us about what prior values we should assign, but otherwise the two models are the same."
  },
  {
    "objectID": "posts/probit-probit/index.html#footnotes",
    "href": "posts/probit-probit/index.html#footnotes",
    "title": "Probing the Depths of Probit Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe standard Normal distribution is a Normal distribution with mean 0 and standard deviation 1.‚Ü©Ô∏é\n\\(\\text{ln}\\left(\\frac{x}{1-x}\\right)\\)‚Ü©Ô∏é\nErik Voeten ‚ÄúData and Analyses of Voting in the UN General Assembly‚Äù Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/probit-probit/index.html#the-probit-model-in-action",
    "href": "posts/probit-probit/index.html#the-probit-model-in-action",
    "title": "Probing the Depths of Probit Regression",
    "section": "The Probit Model in Action",
    "text": "The Probit Model in Action\nLet‚Äôs jump into some real data to check out the probit model in action. We‚Äôre going to use data from the United Nations General Assembly Voting Data R package3 which contains information about how each country voted on particular UN resolutions. Our goal is to build a model which predicts how the United States votes on each resolution.\n\nun &lt;- un_votes |&gt; \n  left_join(un_roll_call_issues, by = \"rcid\") |&gt; \n  left_join(un_roll_calls, by = \"rcid\") |&gt; \n  mutate(vote = case_when(vote == \"yes\" ~ 1,\n                          vote == \"no\" ~ 0,\n                          vote == \"abstain\" ~ 2))\n\nThe first step will be to join the three data sets in the unvotes package together so that we have all the possible variables at our disposal. In order to make our lives easier during the modeling steps we need to recode the outcome variable, vote, such that it takes numeric values: ‚Äúyes‚Äù = 1, ‚Äúno‚Äù = 0, and ‚Äúabstain‚Äù = 2. The basic probit model can only handle binary outcome variables, so we will ignore abstentions for the time being.\n\nun &lt;- un |&gt; \n  group_by(rcid) |&gt; \n  mutate(ussr_vote = ifelse(any(country_code == \"RU\" & vote == 1), 1, 0)) |&gt; \n  ungroup() |&gt; \n  mutate(issue = ifelse(is.na(issue), \"Other\", as.character(issue))) |&gt; \n  filter(country_code == \"US\",\n         date &lt; as.Date(\"1991-12-26\"))\n\nNext, let‚Äôs assemble our predictor variables. Now I‚Äôm no international relations expert, but I conjecture that one of the best ways to predict how the US will vote on a resolution is by looking at how its longtime foe the USSR voted. Naturally this limits our analysis to the time period before Christmas day 1991‚Äîwhich calls into question our use of the word ‚Äúprediction‚Äù. What does it mean to predict events which took place over 30 years ago? Let‚Äôs overlook this detail for the sake of exposition.\nIn addition to a variable for how the USSR voted on resolutions, we will include a variable in the model for the issue, or topic, of each resolution. I created a new category called ‚ÄúOther‚Äù which contains resolutions with missing NA issues so that these observations are still included in the analysis.\n\nun |&gt; \n  janitor::tabyl(issue)\n\n                                issue    n    percent\n         Arms control and disarmament  512 0.11377778\n                          Colonialism  616 0.13688889\n                 Economic development  514 0.11422222\n                         Human rights  438 0.09733333\n Nuclear weapons and nuclear material  409 0.09088889\n                                Other 1478 0.32844444\n                 Palestinian conflict  533 0.11844444\n\n\nNow we can finally fit our model! This is a very simple probit regression so we can use the brms R package. Because all our predictor variables are categorical, we‚Äôll use the 0 + ... formula syntax for index coding. The code prior(normal(0, 2), class = b) sets weakly informative priors for the coefficients on all these variables (notice how on the standard Normal CDF graph above almost all the probability changes occur between the values -2 and 2). We also need to include bernoulli(link = \"probit\") to ensure that brm() knows we want to run a probit model.\n\nfit_probit &lt;- brm(\n  bf(vote ~ 0 + issue + ussr_vote),\n  prior = prior(normal(0, 2), class = b),\n  family = bernoulli(link = \"probit\"),\n  data = un |&gt; filter(vote != 2), # Getting rid of abstentions\n  cores = 4,\n  chains = 4,\n  backend = \"cmdstanr\",\n  silent = 2,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 4.4 seconds.\nChain 2 finished in 4.6 seconds.\nChain 4 finished in 4.7 seconds.\nChain 1 finished in 4.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.7 seconds.\nTotal execution time: 5.1 seconds.\n\n\nLet‚Äôs run a quick posterior-predictive check to make sure there are no glaring issues with our model. The \\(y\\) line in the graph below shows the actual distribution of 1‚Äôs and 0‚Äôs in our data, and the \\(y_{rep}\\) lines show different draws from the posterior predictive distribution.\n\npp_check(fit_probit, ndraws = 100)\n\n\n\n\n\n\n\n\nNice‚Äîthe model is able to generally predict the 1‚Äôs and 0‚Äôs it was trained on."
  },
  {
    "objectID": "posts/probit-probit/index.html#yo-dawg-i-heard-you-like-probit-models",
    "href": "posts/probit-probit/index.html#yo-dawg-i-heard-you-like-probit-models",
    "title": "Probing the Depths of Probit Regression",
    "section": "Yo Dawg I heard you like Probit Models",
    "text": "Yo Dawg I heard you like Probit Models\nNow that we‚Äôre probit model experts, let‚Äôs try something a little more interesting. Remember those abstentions we left out of the previous model? It‚Äôs time to add them back in.\nThere are multiple ways to construct a model with three possible outcomes (yes, no, abstain), but one of the most intuitive is to add a hurdle process to our original probit model. A hurdle model has two parts: first, we model whether some event will take place or not, then, conditional on the event taking place (i.e.¬†overcoming the hurdle), we model the outcome of the event. So in our United Nations resolution example, the hurdle in question is whether a country takes any vote or abstains from voting. Then, if we predict that the country votes at all, we subsequently predict whether it will vote ‚Äúyes‚Äù or ‚Äúno‚Äù on the resolution.\nWe‚Äôve covered how to model whether a country will vote ‚Äúyes‚Äù or ‚Äúno‚Äù, but how do we model whether a country votes or abstains? The decision whether to vote or not to vote is itself a binary outcome‚Äîso we get to use the probit model once again! Two probits in one model! I‚Äôm still trying to come up with a catchy name for this type of hurdle probit model‚ÄîChatGPT was not much help:\n\nQ: What would be a catchy name for a hurdle probit model?\n\n\nA: How about ‚ÄúTrippy Hurdle Probit‚Äù? It combines the idea of a challenging hurdle with a humorous twist, suggesting that the model might encounter some unconventional obstacles along the way.\n\nUnsurprisingly there is no ‚Äútrippy hurdle probit‚Äù regression family in brms. Luckily brms allows you to create your own model families using the custom_family() function. The code for this section was adapted from these two great resources on custom families in brms: Andrew Heiss‚Äôs blog, and brms package vignette.\n\nhurdle_probit &lt;- custom_family(\n  \"hurdle_probit\",\n  dpars = c(\"mu\", \"theta\"),\n  links = c(\"identity\", \"probit\"),\n  type = \"int\")\n\nFirst we define a custom_family() with two distributional parameters, or dpars. The mu parameter corresponds to the yes/no part of the model and theta corresponds to the hurdle part. Unlike ‚Äúmu‚Äù (which is required from brms), there is nothing special about choosing the label ‚Äútheta‚Äù here. We‚Äôre just following some common notation.\n\nstan_funs &lt;- \"\n  real hurdle_probit_lpmf(int y, real mu, real theta) {\n    if (y == 2) {\n      return bernoulli_lpmf(1 | theta);\n    } else {\n      return bernoulli_lpmf(0 | theta) +\n             bernoulli_lpmf(y | Phi(mu));\n    }\n  }\n\"\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\nNext we need to write a new Stan function for brms to use. Note the conditional statement y == 2 which corresponds to an abstention in our original data.\nWhen building a new, and more complicated model like this it is a good idea to run a quick simulation to ensure things are working as expected. Our goal here is to determine whether the model can recover the same parameters we used to generate a synthetic data set. In the simulation code below, these parameters correspond to x_coef and z_coef.\n\nN &lt;- 1000\nx_coef &lt;- -1\nz_coef &lt;- 2\n\nhurdle_sim_data &lt;- tibble(\n  x = rnorm(N), # random variable affecting yes/no\n  z = rnorm(N)  # random variable affecting hurdle\n) |&gt; \n  mutate(pr_abstain = pnorm(z * z_coef),              # probability of voting or abstaining\n         abstain = rbinom(n(), 1, prob = pr_abstain), # binary abstain/vote\n         pr_yes = pnorm(x * x_coef),                  # probability of yes or no\n         yes = rbinom(n(), 1, prob = pr_yes),         # binary yes/no\n         y = case_when(abstain == 1 ~ 2,              # final realized outcome\n                       yes == 1 ~ 1,\n                       yes == 0 ~ 0))\n\nThe synthetic data hurdle_sim_data can now be fed into a model using the custom family we created above.\n\nfit_sim &lt;- brm(\n  bf(y ~ x,      # yes/no part\n     theta ~ z), # hurdle part\n  prior = prior(normal(0, 2), class = b, coef = x) +\n          prior(normal(0, 2), class = b, coef = z, dpar = theta),\n  data = hurdle_sim_data,\n  family = hurdle_probit, # the custom_family we made\n  stanvars = stanvars,    # the Stan function we made\n  backend = \"cmdstanr\",\n  chains = 4,\n  cores = 4,\n  silent = 2,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 2.2 seconds.\nChain 2 finished in 2.1 seconds.\nChain 3 finished in 2.1 seconds.\nChain 4 finished in 2.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.2 seconds.\nTotal execution time: 2.4 seconds.\n\n\n\nfixef(fit_sim)\n\n                   Estimate  Est.Error        Q2.5      Q97.5\nIntercept        0.03443466 0.06680660 -0.09861857  0.1680562\ntheta_Intercept  0.04426272 0.05947186 -0.07504551  0.1635512\nx               -1.06246395 0.08840567 -1.23625175 -0.8991952\ntheta_z          2.13310869 0.12449412  1.89234900  2.3782158\n\n\nTaking a look at the coefficient estimates from our simulation model we see that we get roughly the same values as those that were used to generate the synthetic data! And just to further tie everything together, we can separate the two parts of the full hurdle model out and check the results.\n\n# Using maximum likelihood here to save time\nglm(abstain ~ z, \n    data = hurdle_sim_data, \n    family = binomial(link = \"probit\")) |&gt; \n  broom::tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.0448    0.0583     0.768 4.42e- 1\n2 z             2.13      0.124     17.2   5.72e-66\n\n\nAbove we see the effect of z on whether an observation in our simulated data was marked ‚Äúabstain‚Äù or not. Note that the coefficient estimate is the same as theta_z in the full hurdle model above.\n\nglm(yes ~ x, \n    data = hurdle_sim_data |&gt; filter(abstain == 0), \n    family = binomial(link = \"probit\")) |&gt; \n  broom::tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.0325    0.0660     0.492 6.23e- 1\n2 x            -1.06      0.0906   -11.7   1.31e-31\n\n\nAnd here we see the effect of x on the realized outcome y in the simulated data (after excluding the abstentions). Again, the coefficient estimate in this model nicely replicates the x estimate from the full hurdle model above.\nNow that we‚Äôre confident the hurdle probit model is working as intended, we can fit it to the real-world UN data we prepared earlier. We‚Äôll keep the part of the model predicting yes/no votes the same (i.e.¬†using resolution issue and USSR vote variables as predictors). How should we predict whether the US votes or abstains from a particular resolution? Luckily for us, the unvotes data comes with a variable importantvote. Again, I‚Äôm no international relations expert but maybe if a resolution is deemed to be ‚Äúimportant‚Äù the US will be more likely to weigh in‚Äîfor or against.\n\nfit_hurdle &lt;- brm(\n  bf(vote ~ 0 + issue + ussr_vote, # yes/no model\n     theta ~ importantvote),       # abstain/vote model\n  family = hurdle_probit,\n  stanvars = stanvars,\n  data = un,\n  cores = 4,\n  chains = 4,\n  backend = \"cmdstanr\",\n  silent = 2,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 4 finished in 21.1 seconds.\nChain 2 finished in 21.7 seconds.\nChain 3 finished in 23.6 seconds.\nChain 1 finished in 23.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 22.5 seconds.\nTotal execution time: 23.8 seconds.\n\n\nGenerating draws from the posterior predictive distribution from a custom family model in brms requires another user-defined function. This vignette helps us out with this once again.\n\nposterior_predict_hurdle_probit &lt;- function(i, prep, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  theta &lt;- brms::get_dpar(prep, \"theta\", i = i)\n\n  hu &lt;- runif(prep$ndraws, 0, 1)\n  ifelse(hu &lt; theta, 2, rbinom(prep$ndraws, 1, pnorm(mu)))\n}\n\nNow we can check our results using pp_check()\n\npp_check(fit_hurdle, ndraws = 100)\n\n\n\n\nWe see that the model does an okay job recovering the 0‚Äôs (no), 1‚Äôs (yes), and 2‚Äôs (abstain) from the original data."
  },
  {
    "objectID": "posts/probit-probit/index.html#two-probit-to-quit",
    "href": "posts/probit-probit/index.html#two-probit-to-quit",
    "title": "Probing the Depths of Probit Regression",
    "section": "Two Probit to Quit",
    "text": "Two Probit to Quit\nNow that we‚Äôre probit model experts, let‚Äôs try something a little more interesting. Remember those abstentions we left out of the previous model? It‚Äôs time to add them back in.\nThere are multiple ways to construct a model with three possible outcomes (yes, no, abstain), but one of the most intuitive is to add a hurdle process to our original probit model. A hurdle model has two parts: first, we model whether some event will take place or not, then, conditional on the event taking place (i.e.¬†overcoming the hurdle), we model the outcome of the event. So in our United Nations resolution example, the hurdle in question is whether a country takes any vote or abstains from voting. Then, if we predict that the country votes at all, we subsequently predict whether it will vote ‚Äúyes‚Äù or ‚Äúno‚Äù on the resolution.\nWe‚Äôve covered how to model whether a country will vote ‚Äúyes‚Äù or ‚Äúno‚Äù, but how do we model whether a country votes or abstains? The decision whether to vote or not to vote is itself a binary outcome‚Äîso we get to use the probit model once again! Two probits in one model! I‚Äôm still trying to come up with a catchy name for this type of hurdle probit model‚ÄîChatGPT was not much help:\n\nQ: What would be a catchy name for a hurdle probit model?\n\n\nA: How about ‚ÄúTrippy Hurdle Probit‚Äù? It combines the idea of a challenging hurdle with a humorous twist, suggesting that the model might encounter some unconventional obstacles along the way.\n\nUnsurprisingly there is no ‚Äútrippy hurdle probit‚Äù regression family in brms. Luckily brms allows you to create your own model families using the custom_family() function. The code for this section was adapted from these two great resources on custom families in brms: Andrew Heiss‚Äôs blog, and brms package vignette.\n\nhurdle_probit &lt;- custom_family(\n  \"hurdle_probit\",\n  dpars = c(\"mu\", \"theta\"),\n  links = c(\"identity\", \"probit\"),\n  type = \"int\")\n\nFirst we define a custom_family() with two distributional parameters, or dpars. The mu parameter corresponds to the yes/no part of the model and theta corresponds to the hurdle part. Unlike ‚Äúmu‚Äù (which is required from brms), there is nothing special about choosing the label ‚Äútheta‚Äù here. We‚Äôre just following some common notation.\n\nstan_funs &lt;- \"\n  real hurdle_probit_lpmf(int y, real mu, real theta) {\n    if (y == 2) {\n      return bernoulli_lpmf(1 | theta);\n    } else {\n      return bernoulli_lpmf(0 | theta) +\n             bernoulli_lpmf(y | Phi(mu));\n    }\n  }\n\"\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\nNext we need to write a new Stan function for brms to use. Note the conditional statement y == 2 which corresponds to an abstention in our original data.\nWhen building a new, and more complicated model like this it is a good idea to run a quick simulation to ensure things are working as expected. Our goal here is to determine whether the model can recover the same parameters we used to generate a synthetic data set. In the simulation code below, these parameters correspond to x_coef and z_coef.\n\nN &lt;- 1000\nx_coef &lt;- -1\nz_coef &lt;- 2\n\nhurdle_sim_data &lt;- tibble(\n  x = rnorm(N), # random variable affecting yes/no\n  z = rnorm(N)  # random variable affecting hurdle\n) |&gt; \n  mutate(pr_abstain = pnorm(z * z_coef),              # probability of voting or abstaining\n         abstain = rbinom(n(), 1, prob = pr_abstain), # binary abstain/vote\n         pr_yes = pnorm(x * x_coef),                  # probability of yes or no\n         yes = rbinom(n(), 1, prob = pr_yes),         # binary yes/no\n         y = case_when(abstain == 1 ~ 2,              # final realized outcome\n                       yes == 1 ~ 1,\n                       yes == 0 ~ 0))\n\nThe synthetic data hurdle_sim_data can now be fed into a model using the custom family we created above.\n\nfit_sim &lt;- brm(\n  bf(y ~ x,      # yes/no part\n     theta ~ z), # hurdle part\n  prior = prior(normal(0, 2), class = b, coef = x) +\n          prior(normal(0, 2), class = b, coef = z, dpar = theta),\n  data = hurdle_sim_data,\n  family = hurdle_probit, # the custom_family we made\n  stanvars = stanvars,    # the Stan function we made\n  backend = \"cmdstanr\",\n  chains = 4,\n  cores = 4,\n  silent = 2,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 2.1 seconds.\nChain 1 finished in 2.1 seconds.\nChain 3 finished in 2.1 seconds.\nChain 4 finished in 2.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.1 seconds.\nTotal execution time: 2.3 seconds.\n\n\n\nfixef(fit_sim)\n\n                   Estimate  Est.Error       Q2.5       Q97.5\nIntercept       -0.01391639 0.06639284 -0.1390825  0.11676975\ntheta_Intercept -0.03150056 0.05460410 -0.1343178  0.07642558\nx               -0.93124834 0.08551786 -1.0999510 -0.76858800\ntheta_z          1.98504156 0.11159384  1.7738840  2.21075475\n\n\nTaking a look at the coefficient estimates from our simulation model we see that we get roughly the same values as those that were used to generate the synthetic data! And just to further tie everything together, we can separate the two parts of the full hurdle model out and check the results.\n\n# Using maximum likelihood here to save time\nglm(abstain ~ z, \n    data = hurdle_sim_data, \n    family = binomial(link = \"probit\")) |&gt; \n  broom::tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -0.0315    0.0559    -0.563 5.73e- 1\n2 z             1.99      0.113     17.6   1.19e-69\n\n\nAbove we see the effect of z on whether an observation in our simulated data was marked ‚Äúabstain‚Äù or not. Note that the coefficient estimate is the same as theta_z in the full hurdle model above.\n\nglm(yes ~ x, \n    data = hurdle_sim_data |&gt; filter(abstain == 0), \n    family = binomial(link = \"probit\")) |&gt; \n  broom::tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -0.0156    0.0644    -0.243 8.08e- 1\n2 x            -0.928     0.0817   -11.3   7.64e-30\n\n\nAnd here we see the effect of x on the realized outcome y in the simulated data (after excluding the abstentions). Again, the coefficient estimate in this model nicely replicates the x estimate from the full hurdle model above.\nNow that we‚Äôre confident the hurdle probit model is working as intended, we can fit it to the real-world UN data we prepared earlier. We‚Äôll keep the part of the model predicting yes/no votes the same (i.e.¬†using resolution issue and USSR vote variables as predictors). How should we predict whether the US votes or abstains from a particular resolution? Luckily for us, the unvotes data comes with a variable importantvote. Again, I‚Äôm no international relations expert but maybe if a resolution is deemed to be ‚Äúimportant‚Äù the US will be more likely to weigh in‚Äîfor or against.\n\nfit_hurdle &lt;- brm(\n  bf(vote ~ 0 + issue + ussr_vote, # yes/no model\n     theta ~ importantvote),       # abstain/vote model\n  family = hurdle_probit,\n  stanvars = stanvars,\n  data = un,\n  cores = 4,\n  chains = 4,\n  backend = \"cmdstanr\",\n  silent = 2,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 4 finished in 19.0 seconds.\nChain 3 finished in 19.5 seconds.\nChain 2 finished in 20.1 seconds.\nChain 1 finished in 20.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 19.7 seconds.\nTotal execution time: 20.5 seconds.\n\n\nGenerating draws from the posterior predictive distribution from a custom family model in brms requires another user-defined function. This vignette helps us out with this once again.\n\nposterior_predict_hurdle_probit &lt;- function(i, prep, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  theta &lt;- brms::get_dpar(prep, \"theta\", i = i)\n\n  hu &lt;- runif(prep$ndraws, 0, 1)\n  ifelse(hu &lt; theta, 2, rbinom(prep$ndraws, 1, pnorm(mu)))\n}\n\nNow we can check our results using pp_check()\n\npp_check(fit_hurdle, ndraws = 100)\n\n\n\n\n\n\n\n\nWe see that the model does an okay job recovering the 0‚Äôs (no), 1‚Äôs (yes), and 2‚Äôs (abstain) from the original data."
  },
  {
    "objectID": "posts/probit-probit/index.html#concluding-thoughts",
    "href": "posts/probit-probit/index.html#concluding-thoughts",
    "title": "Probing the Depths of Probit Regression",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nIf you‚Äôre tackling a data problem involving binary outcomes and aiming to explicitly model abstentions, consider taking the trippy hurdle probit model for a spin.\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggdist_3.0.99.9000     unvotes_0.3.0          marginaleffects_0.13.0\n [4] tidybayes_3.0.2.9000   brms_2.17.4            Rcpp_1.0.10           \n [7] lubridate_1.9.2        forcats_1.0.0          stringr_1.5.0         \n[10] dplyr_1.1.0            purrr_1.0.1            readr_2.1.4           \n[13] tidyr_1.3.0            tibble_3.2.1           ggplot2_3.4.1         \n[16] tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n  [1] minqa_1.2.4          colorspace_2.0-3     ellipsis_0.3.2      \n  [4] snakecase_0.11.0     estimability_1.3     markdown_1.4        \n  [7] base64enc_0.1-3      rstudioapi_0.14      farver_2.1.1        \n [10] rstan_2.26.11        svUnit_1.0.6         DT_0.26             \n [13] fansi_1.0.4          mvtnorm_1.1-3        bridgesampling_1.1-2\n [16] codetools_0.2-18     splines_4.1.1        knitr_1.42          \n [19] shinythemes_1.2.0    bayesplot_1.10.0     projpred_2.0.2      \n [22] jsonlite_1.8.4       nloptr_1.2.2.2       broom_1.0.3         \n [25] shiny_1.7.4          compiler_4.1.1       emmeans_1.7.2       \n [28] backports_1.4.1      Matrix_1.3-4         fastmap_1.1.1       \n [31] cli_3.6.1            later_1.3.0          htmltools_0.5.5     \n [34] prettyunits_1.1.1    tools_4.1.1          igraph_1.3.5        \n [37] coda_0.19-4          gtable_0.3.1         glue_1.6.2          \n [40] reshape2_1.4.4       posterior_1.3.1      V8_4.2.0            \n [43] vctrs_0.6.2          nlme_3.1-152         crosstalk_1.2.0     \n [46] tensorA_0.36.2       xfun_0.39            ps_1.7.5            \n [49] lme4_1.1-27.1        timechange_0.1.1     mime_0.12           \n [52] miniUI_0.1.1.1       lifecycle_1.0.3      gtools_3.9.4        \n [55] MASS_7.3-54          zoo_1.8-11           scales_1.2.1        \n [58] colourpicker_1.2.0   hms_1.1.2            promises_1.2.0.1    \n [61] Brobdingnag_1.2-7    parallel_4.1.1       inline_0.3.19       \n [64] shinystan_2.6.0      gamm4_0.2-6          yaml_2.3.7          \n [67] curl_5.0.0           gridExtra_2.3        loo_2.5.1           \n [70] StanHeaders_2.26.11  stringi_1.7.12       dygraphs_1.1.1.6    \n [73] checkmate_2.1.0      boot_1.3-28          pkgbuild_1.4.0      \n [76] cmdstanr_0.5.3       rlang_1.1.0          pkgconfig_2.0.3     \n [79] matrixStats_0.63.0   distributional_0.3.1 evaluate_0.20       \n [82] lattice_0.20-44      labeling_0.4.2       rstantools_2.2.0    \n [85] htmlwidgets_1.6.2    tidyselect_1.2.0     processx_3.8.1      \n [88] plyr_1.8.8           magrittr_2.0.3       R6_2.5.1            \n [91] generics_0.1.3       mgcv_1.8-36          pillar_1.9.0        \n [94] withr_2.5.0          xts_0.12.2           abind_1.4-5         \n [97] janitor_2.1.0        crayon_1.5.2         arrayhelpers_1.1-0  \n[100] utf8_1.2.3           tzdb_0.3.0           rmarkdown_2.21      \n[103] grid_4.1.1           data.table_1.14.6    callr_3.7.3         \n[106] threejs_0.3.3        digest_0.6.31        xtable_1.8-4        \n[109] httpuv_1.6.9         RcppParallel_5.1.5   stats4_4.1.1        \n[112] munsell_0.5.0        shinyjs_2.1.0"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The NYC Rat Index\n\n\n\n\n\n\nBayes\n\n\nGIS\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nBertrand Wilden\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Airbnbs in San Diego\n\n\n\n\n\n\nGIS\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nBertrand Wilden\n\n\n\n\n\n\n\n\n\n\n\n\nProbing the Depths of Probit Regression\n\n\n\n\n\n\nBayes\n\n\nbrms\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nBertrand Wilden\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Height Matter When Running for President?\n\n\n\n\n\n\nBayes\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\nBertrand Wilden\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Bertrand Wilden",
    "section": "Quarto",
    "text": "Quarto\n\nHi! I am a data scientist and PhD Candidate at UC San Diego working on a degree in Political Science with a Specialization in Computational Social Science. My research involves building Bayesian statistical models to measure latent variables such as political ideology. I am also interested in open source software development in R.\nWhen waiting for my MCMC chains to finish I like to rock climb, play board games, and go backpacking."
  },
  {
    "objectID": "posts/president-height/index.html#does-height-matter-when-running-for-president",
    "href": "posts/president-height/index.html#does-height-matter-when-running-for-president",
    "title": "Does Height Matter When Running for President?",
    "section": "",
    "text": "Height is supposed to confer all sorts of advantages in life. Taller people make more money, have an easier time finding romantic partners, and can reach things off the highest shelves without using a step stool. But does height matter when it comes to politics? The topic has been the subject of extensive debate‚Äîso much so that a Wikipedia page was written to provide information on the heights of US presidential candidates. In this post I analyze this debate quantitatively using R and Bayesian regression methods. My results conclusively show that height probably doesn‚Äôt matter much when it comes to winning the presidency.\n\n# Loading in the packages used\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(MetBrewer)\nlibrary(ggdist)\nlibrary(brms)\nlibrary(distributional)\nlibrary(geomtextpath)\nlibrary(tidybayes)\n\n# Global plotting theme for ggplot\ntheme_set(theme_ggdist())\n\n# Set global rounding options\noptions(scipen = 1, \n        digits = 3)"
  },
  {
    "objectID": "posts/probit-probit/index.html#session-info",
    "href": "posts/probit-probit/index.html#session-info",
    "title": "Probing the Depths of Probit Regression",
    "section": "Session Info",
    "text": "Session Info\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggdist_3.0.99.9000     unvotes_0.3.0          marginaleffects_0.13.0\n [4] tidybayes_3.0.2.9000   brms_2.17.4            Rcpp_1.0.10           \n [7] lubridate_1.9.2        forcats_1.0.0          stringr_1.5.0         \n[10] dplyr_1.1.0            purrr_1.0.1            readr_2.1.4           \n[13] tidyr_1.3.0            tibble_3.2.1           ggplot2_3.4.1         \n[16] tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n  [1] minqa_1.2.4          colorspace_2.0-3     ellipsis_0.3.2      \n  [4] snakecase_0.11.0     estimability_1.3     markdown_1.4        \n  [7] base64enc_0.1-3      rstudioapi_0.14      farver_2.1.1        \n [10] rstan_2.26.11        svUnit_1.0.6         DT_0.26             \n [13] fansi_1.0.4          mvtnorm_1.1-3        bridgesampling_1.1-2\n [16] codetools_0.2-18     splines_4.1.1        knitr_1.42          \n [19] shinythemes_1.2.0    bayesplot_1.10.0     projpred_2.0.2      \n [22] jsonlite_1.8.4       nloptr_1.2.2.2       broom_1.0.3         \n [25] shiny_1.7.4          compiler_4.1.1       emmeans_1.7.2       \n [28] backports_1.4.1      Matrix_1.3-4         fastmap_1.1.1       \n [31] cli_3.6.1            later_1.3.0          htmltools_0.5.5     \n [34] prettyunits_1.1.1    tools_4.1.1          igraph_1.3.5        \n [37] coda_0.19-4          gtable_0.3.1         glue_1.6.2          \n [40] reshape2_1.4.4       posterior_1.3.1      V8_4.2.0            \n [43] vctrs_0.6.2          nlme_3.1-152         crosstalk_1.2.0     \n [46] tensorA_0.36.2       xfun_0.39            ps_1.7.5            \n [49] lme4_1.1-27.1        timechange_0.1.1     mime_0.12           \n [52] miniUI_0.1.1.1       lifecycle_1.0.3      gtools_3.9.4        \n [55] MASS_7.3-54          zoo_1.8-11           scales_1.2.1        \n [58] colourpicker_1.2.0   hms_1.1.2            promises_1.2.0.1    \n [61] Brobdingnag_1.2-7    parallel_4.1.1       inline_0.3.19       \n [64] shinystan_2.6.0      gamm4_0.2-6          yaml_2.3.7          \n [67] curl_5.0.0           gridExtra_2.3        loo_2.5.1           \n [70] StanHeaders_2.26.11  stringi_1.7.12       dygraphs_1.1.1.6    \n [73] checkmate_2.1.0      boot_1.3-28          pkgbuild_1.4.0      \n [76] cmdstanr_0.5.3       rlang_1.1.0          pkgconfig_2.0.3     \n [79] matrixStats_0.63.0   distributional_0.3.1 evaluate_0.20       \n [82] lattice_0.20-44      labeling_0.4.2       rstantools_2.2.0    \n [85] htmlwidgets_1.6.2    tidyselect_1.2.0     processx_3.8.1      \n [88] plyr_1.8.8           magrittr_2.0.3       R6_2.5.1            \n [91] generics_0.1.3       mgcv_1.8-36          pillar_1.9.0        \n [94] withr_2.5.0          xts_0.12.2           abind_1.4-5         \n [97] janitor_2.1.0        crayon_1.5.2         arrayhelpers_1.1-0  \n[100] utf8_1.2.3           tzdb_0.3.0           rmarkdown_2.21      \n[103] grid_4.1.1           data.table_1.14.6    callr_3.7.3         \n[106] threejs_0.3.3        digest_0.6.31        xtable_1.8-4        \n[109] httpuv_1.6.9         RcppParallel_5.1.5   stats4_4.1.1        \n[112] munsell_0.5.0        shinyjs_2.1.0"
  },
  {
    "objectID": "posts/sd-stro/index.html",
    "href": "posts/sd-stro/index.html",
    "title": "Mapping Airbnbs in San Diego",
    "section": "",
    "text": "About a week ago on the r/SanDiegan subreddit someone posted a link to new data from the City of San Diego on Short-Term Residential Occupancy (STRO) licenses. These data show the addresses and owners of every licensed Airbnb (and other similar arrangements, I guess) in the city. Airbnb‚Äôs are a soure of ire among some San Diego residents for supposedly wasting our precious housing supply. My view is that this issue is a bit of a red-herring. Housing in California cities like San Diego is so catastrophically under-supplied due to years of restrictive zoning laws that, even if Airbnbs were all made illegal tomorrow, it wouldn‚Äôt make much of a difference.\nIn this post I‚Äôm going to walk through how to make some maps with this STRO data using R. I posted one these maps to Reddit but made an embarrassing error which resulted in the incorrect magnitudes being displayed. Always check your work before posting something online!"
  },
  {
    "objectID": "posts/sd-stro/index.html#introducing-the-data",
    "href": "posts/sd-stro/index.html#introducing-the-data",
    "title": "Mapping Airbnbs in San Diego",
    "section": "Introducing the Data",
    "text": "Introducing the Data\n\n# Packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\n\nOur goal is to create a choropleth map of San Diego with regions shaded according to their proportion of STROs. The packages {dplyr} and {ggplot2} are for some light data manipulation and producing the graphs. Inspired by Michael DeCrescenzo‚Äôs posts on functional programming in R, I use {purrr} for some currying and composition later in this post. The {tidycensus} package is the best way to access US Census data in my opinion. And {sf} and {tigris} are my two favorite GIS packages in R.\nNow let‚Äôs take a look at the data.\n\nstro &lt;- readr::read_csv(\"https://seshat.datasd.org/stro_licenses/stro_licenses_datasd.csv\")\nstro\n\n# A tibble: 7,828 √ó 23\n   license_id address      street_number street_number_fraction street_direction\n   &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;           \n 1 STR-01939L 1633 LAW St‚Ä¶          1633 &lt;NA&gt;                   &lt;NA&gt;            \n 2 STR-02856L 2315 AVENID‚Ä¶          2315 &lt;NA&gt;                   &lt;NA&gt;            \n 3 STR-03110L 13680 VIA C‚Ä¶         13680 &lt;NA&gt;                   &lt;NA&gt;            \n 4 STR-01470L 4291 Corte ‚Ä¶          4291 &lt;NA&gt;                   &lt;NA&gt;            \n 5 STR-02940L 1016 W BRIA‚Ä¶          1016 &lt;NA&gt;                   W               \n 6 STR-00057L 4767 OCEAN ‚Ä¶          4767 &lt;NA&gt;                   &lt;NA&gt;            \n 7 STR-02180L 2138 REED A‚Ä¶          2138 &lt;NA&gt;                   &lt;NA&gt;            \n 8 STR-03888L 4212 Florid‚Ä¶          4212 &lt;NA&gt;                   &lt;NA&gt;            \n 9 STR-00513L 3745 33rd S‚Ä¶          3745 &lt;NA&gt;                   &lt;NA&gt;            \n10 STR-02250L 3820 Vermon‚Ä¶          3820 &lt;NA&gt;                   &lt;NA&gt;            \n# ‚Ñπ 7,818 more rows\n# ‚Ñπ 18 more variables: street_name &lt;chr&gt;, street_type &lt;chr&gt;, unit_type &lt;chr&gt;,\n#   unit_number &lt;chr&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;dbl&gt;, tier &lt;chr&gt;,\n#   community_planning_area &lt;chr&gt;, date_expiration &lt;date&gt;, rtax_no &lt;chr&gt;,\n#   tot_no &lt;dbl&gt;, longitude &lt;dbl&gt;, latitude &lt;dbl&gt;,\n#   local_contact_contact_name &lt;chr&gt;, local_contact_phone &lt;dbl&gt;,\n#   host_contact_name &lt;chr&gt;, council_district &lt;dbl&gt;\n\n\nLooks like we‚Äôve got around 7828 STRO licenses currently active in San Diego. But where are they concentrated? Luckily for our geo-spatial aspirations, the longitude and latitude values for these addresses are already contained in the data. If longitude and latitude weren‚Äôt included we would have to use a tool like the Census geocoder or plug the addresses into ArcGIS. These longitude and latitude values will let us figure out in which Census tract these addresses are located, thereby allowing us to map their density.1"
  },
  {
    "objectID": "posts/sd-stro/index.html#geo-spatial-merging",
    "href": "posts/sd-stro/index.html#geo-spatial-merging",
    "title": "Mapping Airbnbs in San Diego",
    "section": "Geo-spatial Merging",
    "text": "Geo-spatial Merging\nThe first step in linking addresses to tracts is loading in a shapefile containing the boundaries of the Census tracts in San Diego county. The {tigris} package conveniently has a tracts() function which gives us exactly what we need. We are also going to save the coordinate-reference-system (CRS) of this tracts object for later use. Dealing with CRS‚Äôs is the source of a lot of GIS headaches. Without a common CRS, the various data sources we‚Äôre assembling in this project won‚Äôt be able to spatially match up with one another. The target_crs object will help us deal with this issue.\n\nsd_tracts &lt;- tracts(state = \"CA\", county = \"San Diego\",\n                    progress_bar = FALSE)\n\nRetrieving data for the year 2021\n\ntarget_crs &lt;- st_crs(sd_tracts)\n\nNow we want to use st_join() to match the census tracts in sd_tracts with the addresses in our stro data frame. But first we need to convert the stro object into a shapefile using st_as_sf(). Note how we set crs = target_crs below to ensure that the shapefile version of stro is using the same CRS as our tracts data. The st_as_sf() function will error if any rows are missing coordinates, so we filter out the NA addresses first (these mostly seem to be duplicates in the original STRO data for some reason). Now we‚Äôre all set to st_join() in the tracts data, thereby matching each STRO address with the Census tract in which it is located.\n\nstro_geo &lt;- stro |&gt; \n  filter(!is.na(longitude)) |&gt; \n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n           crs = target_crs,\n           remove = FALSE) |&gt;\n  st_join(sd_tracts)\n\nThe variable for Census tract in the stro_geo shapefile/data frame is GEOID. This is an 11 digit value comprised of the two-digit state code (‚Äú06‚Äù for California), followed by the three-digit county code (‚Äú073‚Äù for San Diego county), followed by a six-digit tract code. We‚Äôll perform a simple group-by and summarise operation to calculate the total number of STRO licenses in each tract using this GEOID variable. The st_drop_geometry() function gets rid of the spatial boundaries for each tract in our data. We don‚Äôt need those anymore because we‚Äôll be merging by GEOID from now on.\n\nstro_geo &lt;- stro_geo |&gt; \n  group_by(GEOID) |&gt; \n  summarise(total_licenses = n()) |&gt; \n  st_drop_geometry()\nstro_geo\n\n# A tibble: 301 √ó 2\n   GEOID       total_licenses\n * &lt;chr&gt;                &lt;int&gt;\n 1 06073000100             22\n 2 06073000201             23\n 3 06073000202             74\n 4 06073000301             37\n 5 06073000302             18\n 6 06073000400             26\n 7 06073000500             49\n 8 06073000600             40\n 9 06073000700             61\n10 06073000800             55\n# ‚Ñπ 291 more rows\n\n\nIf we were to map out the total_licenses variable as it stands, we would probably end up with something that looks basically like a population map. Tracts with more total housing units will simply have more STRO permits to give out. Instead, we need to divide the total licenses in each tract by the total housing units, thereby giving us the proportion of housing devoted to STRO licenses. We can get the total housing units, by tract, using the get_acs() function in the {tidycensus} package. In order to grab Census data in this way, you will have to sign up for a Census API key (see here for more information). The variable for housing units in the American Community Survey (ACS) is ‚ÄúB25001_001E‚Äù, which we can rename ‚Äútotal_housing_units‚Äù within the API call.2\n\nsd_acs &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\"total_housing_units\" = \"B25001_001E\"),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = FALSE\n)\n\nGetting data from the 2017-2021 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nWe‚Äôre also going to include geometry = TRUE in our API call so we get the tract boundaries for plotting later. Unfortunately, these geometries are quite coarse and ‚Äúblocky‚Äù. If we want to see all of the nice intricate geographic details in San Diego‚Äôs Mission Bay, for example, we‚Äôll need to use the erase_water() function from {tigris}. For some reason erase_water() messes up the Census shapefile geometries but we can fix that with {sf}‚Äôs st_make_valid().\n\nsd_acs &lt;- sd_acs |&gt; \n  st_transform() |&gt;\n  erase_water(year = 2021) |&gt; \n  st_make_valid() # Water makes the geometries wonky\n\nFetching area water data for your dataset's location...\n\n\nErasing water area...\nIf this is slow, try a larger area threshold value.\n\n\nThe last step before making some cool maps is to join our ACS data to the geocoded STRO data. The argument geography = \"tract\" in get_acs() above ensures that the GEOIDs in that data will match up with the tract GEOIDs in our STRO data. If we left-join the STRO data into the ACS data we will keep all tracts in San Diego in the resulting data frame. Tracts without any STRO licenses will have NA values for the total_licenses variable, which we can turn into 0‚Äôs with tidyr::replace_na().\n\nsd &lt;- sd_acs |&gt; \n  left_join(stro_geo, by = \"GEOID\") |&gt; \n  mutate(total_licenses = tidyr::replace_na(total_licenses, 0),\n         prop_stro = total_licenses / total_housing_units,\n         log_prop_stro = log(prop_stro))\n\nThen we can calculate tract-level STRO proportions by dividing the total_licenses by the total_housing_units. We also want to generate a variable for the log of this proportion. As Figure¬†1 shows, STRO licenses are heavily skewed towards certain neighborhoods. Up to 38.7% of housing units in some beach areas of San Diego are devoted to STROs, whereas I‚Äôm not sure why anyone would want to rent an Airbnb in Kearny Mesa. Taking the natural log of our STRO proportion variable will help show distinctions much more clearly when it comes to making the choropleth map.\n\n\n\n\n\n\n\n\nFigure¬†1: STRO Proportion Tract Histogram"
  },
  {
    "objectID": "posts/sd-stro/index.html#mapping-functions",
    "href": "posts/sd-stro/index.html#mapping-functions",
    "title": "Mapping Airbnbs in San Diego",
    "section": "Mapping Functions",
    "text": "Mapping Functions\nIt‚Äôs almost time to make some maps! We will be making several versions using the same basic template, so this calls for writing our own function. The custom function make_log_prop_stro_map() below takes a data frame as input (our sd object in this case) and outputs a beautiful choropleth map with Census tracts shaded according to their STRO proportion. Most of the heavy lifting in this function is done by geom_sf(). This function recognizes the spatial geometries in our data and draws the borders for each tract. The argument lwd controls the thickness of these borders. I tried playing around with this option so that the border thickness is a function of the number of tracts in the input data. The more tracts, the thinner the border should be so that things don‚Äôt get too crowded. This will be relevant in a minute when we zoom in on different areas of San Diego.\n\nmake_log_prop_stro_map &lt;- function(input_data) {\n  p &lt;- ggplot(input_data) +\n    aes(fill = log_prop_stro) +\n    geom_sf(color = \"black\", lwd = 50 / nrow(input_data)) +\n    theme_void() +\n    scale_fill_viridis_log_prop_stro() +\n    labs(title = \"Proportion of Short Term Rental Licenses\\nby Total Households per Census Tract\")\n  return(p)\n}\n\nscale_fill_viridis_log_prop_stro &lt;- partial(\n  scale_fill_viridis_c, \n  labels = function(x) round(exp(x), 3),\n  breaks = log(c(0.005, 0.02, 0.1, .30)),\n  name = \"Proportion\",\n  option = \"B\",\n  na.value = \"grey\")\n\nYou might be wondering what‚Äôs going on with scale_fill_viridis_log_prop_stro(). This is another function we are writing in order to control the color palette and legend in our map. The partial() function from the {purrr} package takes a function and returns a new function which is a copy of the original function with new default arguments. In this case, we‚Äôre taking {ggplot2}‚Äôs scale_fill_viridis_c() and adding some options which suit the type of map made by make_log_prop_stro_map(). One of the key arguments is labels = function(x) round(exp(x), 3). By exponentiating our logged proportion STRO variable, we ensure that the legend labels on our map will be in the original, non-logged units. Using ‚Äúpartial‚Äù functions opens up a lot of possibilities in your coding. We could now use scale_fill_viridis_log_prop_stro() in other similar plots without copying and pasting a bunch of specific scale_fill_viridis_c() arguments if we wanted to. Read about this here if you‚Äôre interested in learning more about partial, or curried functions."
  },
  {
    "objectID": "posts/sd-stro/index.html#the-maps",
    "href": "posts/sd-stro/index.html#the-maps",
    "title": "Mapping Airbnbs in San Diego",
    "section": "The Maps",
    "text": "The Maps\nOkay now it is FINALLY time to make some maps!\n\nmake_log_prop_stro_map(sd)\n\n\n\n\n\n\n\nFigure¬†2: San Diego County\n\n\n\n\n\nWow, San Diego county is really big and we only have data for a small part of it. All those grey shaded regions either have zero STROs or they are not included in the San Diego city data set we started with. Perhaps if we squint really hard we can make out some geographic patterns.\nWouldn‚Äôt it be nice if we could zoom in a bit? I experimented with a lot of different methods for accomplishing this, such as restricting the Census data to tracts in the place of San Diego city, but the easiest method I found involved using st_crop() to filter out data which lies outside a box defined by four longitude and latitude points. To find the desired longitude and latitude points, you can use something like Google maps or you can make the map using theme_bw() as shown in Figure¬†3 and go from there.\n\nmake_log_prop_stro_map(sd) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†3: San Diego County\n\n\n\n\n\nIn keeping with our functional programming style, we‚Äôll write some curried functions to accomplish the cropping. The functions st_crop_sd() and st_crop_central_sd() use my painstakingly chosen longitude and latitude values as defaults in the st_crop() function.\n\nst_crop_sd &lt;- partial(\n  st_crop, \n  xmin = -117.3, xmax = -116.99,\n  ymin = 33, ymax = 32.4\n)\n\nst_crop_central_sd &lt;- partial(\n  st_crop, \n  xmin = -117.3, xmax = -117,\n  ymin = 32.88, ymax = 32.67)\n\nThere are a few ways we could use our new crop functions. We could take the sd data frame, pipe it into st_crop_sd() then pipe that into make_log_prop_stro_map(). But instead we are going to try to fully embrace a functional programming style approach by using function composition to make our final maps. The compose() function from {purrr} allows us to combine two functions together, evaluating one and then the other. The code below applies the st_crop_sd function to the sd object and then applies make_log_prop_stro_map() to the cropped data frame.3 The result is Figure¬†4. Nice, we successfully filtered out all those Census tracts in San Diego county with no data, thereby showing the STRO density much more clearly!\n\ncompose(make_log_prop_stro_map, st_crop_sd)(sd)\n\n\n\n\n\n\n\nFigure¬†4: San Diego City\n\n\n\n\n\nWe can zoom in even further with st_crop_central_sd() in Figure¬†5. Looks like there are a ton of Airbnbs in Mission Bay and parts of Pacific Beach. Obviously these are desirable vacation spots in San Diego, but these neighborhoods are also comprised mostly of 1-2 story apartments/houses. If we legalized denser housing here maybe more people could live within walking distance of the beach. Wouldn‚Äôt that be nice!\n\ncompose(make_log_prop_stro_map, st_crop_central_sd)(sd)\n\n\n\n\n\n\n\nFigure¬†5: Central San Diego\n\n\n\n\n\nAnd that‚Äôs how you can make nice-looking choropleth maps in R using geocoded data.\nSo is Airbnb the big villain some make it out to be? Using the Census API call below we see that San Diego City has 545,792 total housing units. The STRO data contain around 7828 licenses, which comes out to 1.4% of the total housing stock. It‚Äôs going to take a lot more supply than that to make housing affordable in San Diego. Instead of focusing on Airbnbs, maybe we should work towards supporting things that could really make a difference like SB-10.\n\nget_acs(\n  geography = \"place\",\n  variables = c(\"total_housing_units\" = \"B25001_001E\"),\n  state = \"CA\",\n  output = \"wide\"\n) |&gt; \n  filter(NAME == \"San Diego city, California\")\n\n# A tibble: 1 √ó 4\n  GEOID   NAME                       total_housing_units B25001_001M\n  &lt;chr&gt;   &lt;chr&gt;                                    &lt;dbl&gt;       &lt;dbl&gt;\n1 0666000 San Diego city, California              545792        2846\n\n\n\nsessionInfo()\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tigris_2.0.3     sf_1.0-9         tidycensus_1.4.1 purrr_1.0.1     \n[5] ggplot2_3.4.1    dplyr_1.1.0     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10        tidyr_1.3.0        class_7.3-19       digest_0.6.31     \n [5] utf8_1.2.3         R6_2.5.1           evaluate_0.20      e1071_1.7-12      \n [9] httr_1.4.5         pillar_1.9.0       rlang_1.1.0        curl_5.0.0        \n[13] uuid_1.1-0         rstudioapi_0.14    rmarkdown_2.21     labeling_0.4.2    \n[17] readr_2.1.4        stringr_1.5.0      htmlwidgets_1.6.2  bit_4.0.5         \n[21] munsell_0.5.0      proxy_0.4-27       compiler_4.1.1     xfun_0.39         \n[25] pkgconfig_2.0.3    htmltools_0.5.5    tidyselect_1.2.0   tibble_3.2.1      \n[29] fansi_1.0.4        viridisLite_0.4.1  crayon_1.5.2       tzdb_0.3.0        \n[33] withr_2.5.0        wk_0.7.1           rappdirs_0.3.3     grid_4.1.1        \n[37] jsonlite_1.8.4     gtable_0.3.1       lifecycle_1.0.3    DBI_1.1.3         \n[41] magrittr_2.0.3     units_0.8-1        scales_1.2.1       KernSmooth_2.23-20\n[45] cli_3.6.1          stringi_1.7.12     vroom_1.6.0        farver_2.1.1      \n[49] xml2_1.3.3         ellipsis_0.3.2     generics_0.1.3     vctrs_0.6.2       \n[53] s2_1.1.1           tools_4.1.1        bit64_4.0.5        glue_1.6.2        \n[57] hms_1.1.2          parallel_4.1.1     fastmap_1.1.1      yaml_2.3.7        \n[61] colorspace_2.0-3   classInt_0.4-8     rvest_1.0.3        knitr_1.42"
  },
  {
    "objectID": "posts/sd-stro/index.html#footnotes",
    "href": "posts/sd-stro/index.html#footnotes",
    "title": "Mapping Airbnbs in San Diego",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs someone mentioned in my Reddit post, there are alternative ways to map the density of geo-spatial data‚Äîsuch as plotting the points directly on the map. The way I‚Äôm doing it here runs the risk of running into the Modifiable areal unit problem‚Ü©Ô∏é\nIn the version of the final map I posted to Reddit, I accidentally used the ACS variable for total households. This corresponds to sets of people living in each Census tract, rather than physical housing units. Total physical housing units will generally be more than total households because many units will be vacant at any given time. The two variables, however, appear to be roughly proportional to one another. So the map I originally posted was incorrect in terms of magnitude but shows a similar pattern of STRO density as the corrected map.‚Ü©Ô∏é\nAgain, Michael DeCrescenzo has a nice post on function compostion here: https://mikedecr.netlify.app/blog/composition/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/nyc-rats/index.html",
    "href": "posts/nyc-rats/index.html",
    "title": "The NYC Rat Index",
    "section": "",
    "text": "Rats are ‚Äúpublic enemy number one‚Äù‚Äîat least according to New York City Mayor Eric Adams. Last year the city established a ‚ÄúRat Czar‚Äù who has been tasked with detecting and exterminating rat populations across the five boroughs. While I respect the lives all creatures great and small, mapping out the concentration of rats in the city seems like a worthwhile public service.\nIn this project I develop the NYC Rat Index using geospatial analysis in R. We will walk through some GIS wrangling steps and then employ Bayesian modeling to measure rat activity across New York. Use these results to figure out which neighborhoods to avoid‚Äîor which to seek out‚Äîdepending on your overall disposition towards wild rodents.\n# Packages used:\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(tidycensus)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(MetBrewer)\nlibrary(spdep)\nlibrary(sf)\nlibrary(INLA)\nlibrary(leaflet)"
  },
  {
    "objectID": "posts/nyc-rats/index.html#footnotes",
    "href": "posts/nyc-rats/index.html#footnotes",
    "title": "The NYC Rat Index",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Manhattan, 42 buildings have their own ZIP code.‚Ü©Ô∏é\nNo rats were poissoned during the fitting of this model.‚Ü©Ô∏é"
  }
]