{
  "hash": "00cf55d11d66ecb56dc52ab54ac3410d",
  "result": {
    "markdown": "---\ntitle: \"Probit and Hurdle Probit Modeling\"\nauthor: \"Bertrand Wilden\"\ndate: \"2023-01-10\"\ncategories: [Bayes]\nmax-description-length: 20\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Packages and Global Options\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\nlibrary(unvotes)\nlibrary(ggdist)\n\ntheme_set(theme_minimal())\n```\n:::\n\n\nWhen do you use a probit model in statistics? When you have some data and want to probe it for answers! It is a method for modeling a data generating process which results in a binary 0/1 outcome. Maybe you are trying to explain whether someone votes in a particular election, whether a web user clicks on a link, or whether writing statistics blog posts helps its author get a job. Probit models are closely related to their more popular cousin: the logistic, or logit regression. So closely related, in fact, that I can't think of any decisive reason why someone would choose one over the other. I like probit regression because of the pun potential. I always aspire to maintain a high level of statistical probity in my work. In this post I am going to explain what a probit model is introduce a brand new nested probit model.\n\n## What is a Probit Model?\n\nAs I mentioned before, we can use a probit model when the outcome of interest is some binary variable. Binary outcomes arise out of what's known as a *Bernoulli* distribution, which we write as:\n\n$$\ny_i \\sim \\text{Bernoulli}(p_i)\n$$\n\nThe $y_i$ above stands for the observed 1's and 0's in our data, and the $p_i$ is the probability of a particular $y_i$ equaling 1. For example, the heads (1) and tails (0) we observe from flipping a coin repeatedly would be generated from a Bernoulli distribution with $p_i = 0.5$. We know that $p_i = 0.5$ in the coin flipping example because we have no information that could lead us to expect one outcome over another---hence a 50/50 probability of getting heads. For more complicated data generating processes, however, we don't know $p_i$ in advance. Instead, we use statistical models to rank the relative plausibility of every possible value of $p_i$ based on the data we have collected. The general term for this sort of procedure is called Bayesian updating.\n\nHow do we go about constructing plausibility rankings for $p_i$? This is where our probit (*prob*ability un*it*) model comes in. Say we have a number of observed variables, $X_{1i}, X_{2i}, X_{3i}$ whose linear combination we think affects whether $y_i = 1$ or $y_i = 0$ (in other words, $p_i$). In this case we would like to use an equation that looks like:\n\n$$\np_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i}\n$$\n\nHere the $\\beta$ terms represent the marginal effect each of the $X$ variables has on $p_i$. But wait! We defined $p_i$ earlier as a probability value---a real number between 0 and 1. The Bernoulli distribution can't give us values for $y_i$ if $p_i$ is not a valid probability. There is nothing in the equation above that enforces the value of the right hand side, $\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i}$ to be between 0 and 1 üôÅ. Luckily there is a little guy known as $\\Phi$ who is ready to come to our rescue. We simply wrap our right-hand expression in $\\Phi$'s loving embrace and it takes care of transforming the value of these linear predictors to a value on the probability scale of $p_i$:\n\n$$\np_i = \\Phi(\\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\beta_3X_{3i})\n$$\n\nWhat is $\\Phi$ and how does it work? It represents the cumulative distribution function (CDF) for a standard Normal distribution, which means that it takes as input any real number and gives you back the probability of getting that value or less out of a standard Normal distribution.[^1] The graph below shows what this function looks like.\n\n[^1]: The standard Normal distribution is a Normal distribution with mean 0 and standard deviation 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_function(fun = pnorm,\n                xlim = c(-4, 4)) +\n  geom_segment(aes(x = 1.5, xend = 1.5, y = 0, yend = pnorm(1.5)),\n                   color = \"red\", linetype = \"dashed\") +\n  geom_segment(aes(x = -4, xend = 1.5, y = pnorm(1.5), yend = pnorm(1.5)),\n                   color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(-4, 4, by = .5)) +\n  scale_y_continuous(breaks = seq(0, 1, by = .1)) +\n  labs(title = \"Standard Normal CDF\",\n       x = \"x\",\n       y = \"Cumulative Probability\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we supply $\\Phi$ with an input of 1.5, it gives us an output of 0.93.\n\nThe standard Normal CDF's ability to turn any real number into a probability between 0 and 1 is what allows us to build a model for $p_i$, which then tells us something about the Bernoulli distributions which produced our binary outcome variable $y_i$. As an aside, the difference between probit and logit models comes down to how they transform the linear predictor to the probability scale. Probit uses $\\Phi$ and logit uses something called a log-odds transformation.[^2] This changes the scale of the $\\beta$ parameters, and therefore informs us about the scale of priors we should use in our regression, but otherwise the two models are the same.\n\n[^2]: $\\text{ln}\\left(\\frac{x}{1-x}\\right)$\n\n## The Probit Model in Action\n\nLet's jump into some real data to check out the probit model in action. We're going to use data from the [United Nations General Assembly Voting Data R package](https://github.com/dgrtwo/unvotes)[^3] which contains information about how each country voted on particular UN resolutions. Our goal is to build a model which predicts how the United States votes on each resolution.\n\n[^3]: Erik Voeten \"Data and Analyses of Voting in the UN General Assembly\" Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nun <- un_votes |> \n  left_join(un_roll_call_issues, by = \"rcid\") |> \n  left_join(un_roll_calls, by = \"rcid\") |> \n  mutate(vote = case_when(vote == \"yes\" ~ 1,\n                          vote == \"no\" ~ 0,\n                          vote == \"abstain\" ~ 2))\n```\n:::\n\n\nThe first step will be to join the three data sets in the **unvotes** package together so that we have all the possible variables at our disposal. In order to make our lives easier during the modeling steps we need to recode the outcome variable, `vote`, such that it takes numeric values: \"yes\" = 1, \"no\" = 0, and \"abstain\" = 2. The basic probit model can only handle binary outcome variables, so we will ignore abstentions for the time being.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nun <- un |> \n  group_by(rcid) |> \n  mutate(ussr_vote = ifelse(any(country_code == \"RU\" & vote == 1), 1, 0)) |> \n  ungroup() |> \n  mutate(issue = ifelse(is.na(issue), \"Other\", as.character(issue))) |> \n  filter(country_code == \"US\",\n         date < as.Date(\"1991-12-26\"))\n```\n:::\n\n\nNext, let's assemble our predictor variables. Now I'm no international relations expert, but I conjecture that one of the best ways to predict how the US will vote on a resolution is by looking at how its longtime foe the USSR voted. Naturally this limits our analysis to the time period before Christmas day 1991---which calls into question our use of the word \"prediction\". What does it mean to predict events which took place over 30 years ago? Let's just overlook this detail for the sake of exposition.\n\nIn addition to a variable for how the USSR voted on resolutions, we will include a variable in the model for the issue, or topic, of each resolution. I created a new category called \"Other\" which contains resolutions with missing `NA` issues so that these observations are still included in the analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nun |> \n  janitor::tabyl(issue)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\n‚Ñπ Please use one dimensional logical vectors instead.\n‚Ñπ The deprecated feature was likely used in the dplyr package.\n  Please report the issue at <https://github.com/tidyverse/dplyr/issues>.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                issue    n    percent\n         Arms control and disarmament  512 0.11377778\n                          Colonialism  616 0.13688889\n                 Economic development  514 0.11422222\n                         Human rights  438 0.09733333\n Nuclear weapons and nuclear material  409 0.09088889\n                                Other 1478 0.32844444\n                 Palestinian conflict  533 0.11844444\n```\n:::\n:::\n\n\nNow we can finally fit our model! This is a very simple probit regression so we can use the [brms R package](https://paul-buerkner.github.io/brms/). Because all our predictor variables are categorical, we'll use the `0 + ...` formula syntax for [index coding](https://bookdown.org/content/4857/the-many-variables-the-spurious-waffles.html#categorical-variables). The code `prior(normal(0, 2), class = b)` sets weakly informative priors for the coefficients on all these variables (notice how on the standard Normal CDF graph above almost all the probability changes occur between the values -2 and 2). We include `bernoulli(link = \"probit\")` to ensure `brm` knows we want to run a probit model.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_9cfa70857636d3823de9487911b6c942'}\n\n```{.r .cell-code}\nfit_probit <- brm(\n  bf(vote ~ 0 + issue + ussr_vote),\n  prior = prior(normal(0, 2), class = b),\n  family = bernoulli(link = \"probit\"),\n  data = un |> filter(vote != 2), # Getting rid of abstentions\n  cores = 4,\n  chains = 4,\n  backend = \"cmdstanr\",\n  silent = 2,\n  refresh = 0\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Rows containing NAs were excluded from the model.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 4.5 seconds.\nChain 2 finished in 4.6 seconds.\nChain 3 finished in 4.8 seconds.\nChain 4 finished in 4.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.7 seconds.\nTotal execution time: 5.0 seconds.\n```\n:::\n:::\n\n\nLet's run a quick posterior-predictive check to make sure there are no glaring issues with our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(fit_probit, ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNice---the model is able to generally predict the 1's and 0's it was trained on.\n\n\n\n\n\n\n## Yo Dawg I heard you like Probit Models\n\nNow that we're probit model experts, let's try something a little more interesting. Remember those abstentions we left out of the previous model? It's time to add them back in. \n\nThere are multiple ways to construct a model with three possible outcomes (yes, no, abstain), but one of the most intuitive is to add a hurdle process to our original probit model. A hurdle model has two parts: first, we model whether some event will take place or not, then, conditional on the event taking place (aka overcoming the hurdle), we model the outcome of the event. So in our United Nations resolution example, the hurdle in question is whether a country takes any vote or abstains from voting. Then, if we predict that the country votes at all, we predict whether it will vote \"yes\" or \"no\" on the resolution.\n\nWe've covered how to model whether a country will vote \"yes\" or \"no\", but how do we model whether a country votes or abstains? The decision whether to vote or not to vote is itself a binary outcome---so we get to use the probit model once again! Two probits in one model! I'm still trying to come up with a catchy name for this type of hurdle probit model---ChatGPT was not much help:\n\n> Q: What would be a catchy name for a hurdle probit model?\n\n> A: How about \"Trippy Hurdle Probit\"? It combines the idea of a challenging hurdle with a humorous twist, suggesting that the model might encounter some unconventional obstacles along the way.\n\nUnsurprisingly there is no \"trippy hurdle probit\" regression family in **brms**. Luckily **brms** allows you to create your own model families using the `custom_family()` function. The code for this section was adapted from these two great resources on custom families in **brms**: [Andrew Heiss's blog](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-gaussian-model-with-a-custom-brms-family), and [brms package vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhurdle_probit <- custom_family(\n  \"hurdle_probit\",\n  dpars = c(\"mu\", \"theta\"),\n  links = c(\"identity\", \"probit\"),\n  type = \"int\")\n```\n:::\n\n\nFirst we define a `custom_family()` with two distributional parameters, or `dpars`. The `mu` parameter corresponds to the yes/no part of the model and `theta` corresponds to the hurdle part. There is nothing special about choosing the label \"theta\" here. We're just following some [common notation](https://mc-stan.org/docs/2_20/stan-users-guide/zero-inflated-section.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_funs <- \"\n  real hurdle_probit_lpmf(int y, real mu, real theta) {\n    if (y == 2) {\n      return bernoulli_lpmf(1 | theta);\n    } else {\n      return bernoulli_lpmf(0 | theta) +\n             bernoulli_lpmf(y | Phi(mu));\n    }\n  }\n\"\nstanvars <- stanvar(scode = stan_funs, block = \"functions\")\n```\n:::\n\n\nNext we need to write a new Stan function for **brms** to use. Note the conditional statement `y == 2` which corresponds to an abstention in our original data.\n\nWhen building a new, and more complicated model like this it is a good idea to run a quick simulation to ensure things are working as expected. Our goal here is to determine whether the model can recover the parameters we used to generate a synthetic data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1000\nx_coef <- -1\nz_coef <- 2\n\nhurdle_sim_data <- tibble(\n  x = rnorm(N), # random variable affecting yes/no\n  z = rnorm(N) # random variable affecting hurdle\n) |> \n  mutate(# probability of voting or abstaining\n         pr_abstain = pnorm(z * z_coef), \n         # convert probability to binary abstain/vote\n         abstain = rbinom(n(), 1, prob = pr_abstain),\n         # probability of yes or no\n         pr_yes = pnorm(x * x_coef),\n         # convert probability to yes/no\n         yes = rbinom(n(), 1, prob = pr_yes),\n         # final realized outcome\n         y = case_when(abstain == 1 ~ 2,\n                       yes == 1 ~ 1,\n                       yes == 0 ~ 0))\n```\n:::\n\n\nThe synthetic data `hurdle_sim_data` can now be fed into a model using the custom family we created above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_sim <- brm(\n  bf(y ~ x,      # yes/no part\n     theta ~ z), # hurdle part\n  prior = prior(normal(0, 2), class = b, coef = x) +\n          prior(normal(0, 2), class = b, coef = z, dpar = theta),\n  data = hurdle_sim_data,\n  family = hurdle_probit, # the custom_family we made\n  stanvars = stanvars,    # the Stan function we made\n  backend = \"cmdstanr\",\n  chains = 4,\n  cores = 4,\n  silent = 2,\n  refresh = 0\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 1.8 seconds.\nChain 1 finished in 2.0 seconds.\nChain 4 finished in 2.0 seconds.\nChain 2 finished in 2.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 2.3 seconds.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(fit_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Estimate  Est.Error        Q2.5      Q97.5\nIntercept        0.06125461 0.06566792 -0.07042459  0.1900578\ntheta_Intercept  0.07887222 0.05561422 -0.03125540  0.1889272\nx               -1.07348523 0.09089911 -1.25100400 -0.8989158\ntheta_z          2.01249966 0.11690923  1.78624150  2.2426308\n```\n:::\n:::\n\n\nTaking a look at the coefficient estimates from our simulation model we see that we get roughly the same values as those that were used to generate the synthetic data! And just to further tie everything together, we can separate the two parts of the full hurdle model out and check the results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using maximum likelihood here to save time\nglm(abstain ~ z, \n    data = hurdle_sim_data, \n    family = binomial(link = \"probit\")) |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   0.0799    0.0555      1.44 1.50e- 1\n2 z             2.01      0.118      17.1  1.87e-65\n```\n:::\n:::\n\n\nAbove we see the effect of `z` on whether a row in our simulated data was marked \"abstain\". The coefficient is the same as `theta_z` in the hurdle model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(yes ~ x, \n    data = hurdle_sim_data |> filter(abstain == 0), \n    family = binomial(link = \"probit\")) |> \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   0.0596    0.0661     0.902 3.67e- 1\n2 x            -1.07      0.0901   -11.9   1.41e-32\n```\n:::\n:::\n\n\nAnd here we see the effect of `x` on the realized outcome `y` in the simulated data (after excluding the abstentions). The coefficient estimate in this model nicely replicates the `x` estimate on the hurdle model.\n\nNow that we're confident the hurdle probit model is working as intended, we can fit it to the real-world UN data we prepared earlier. We'll keep the part of the model predicting yes/no votes the same (i.e. using resolution issue and USSR vote variables as predictors). How should we predict whether the US votes or abstains from a particular resolution? Luckily for us, the **unvotes** data comes with a variable `importantvote`. Again, I'm no international relations expert but maybe if a resolution is deemed to be \"important\" the US will be more likely to weigh in---for or against.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-16_343a50046e64808f338a0adbb4d10fce'}\n\n```{.r .cell-code}\nfit_hurdle <- brm(\n  bf(vote ~ 0 + issue + ussr_vote, # yes/no model\n     theta ~ importantvote),       # abstain/vote model\n  family = hurdle_probit,\n  stanvars = stanvars,\n  data = un,\n  cores = 4,\n  chains = 4,\n  backend = \"cmdstanr\",\n  silent = 2,\n  refresh = 0\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Rows containing NAs were excluded from the model.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 21.3 seconds.\nChain 3 finished in 23.0 seconds.\nChain 4 finished in 25.0 seconds.\nChain 1 finished in 25.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 23.7 seconds.\nTotal execution time: 25.6 seconds.\n```\n:::\n:::\n\n\nGenerating draws from the posterior predictive distribution from a custom family model in **brms** requires another user-defined function. [This vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html) helps us out with this once again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_predict_hurdle_probit <- function(i, prep, ...) {\n  mu <- brms::get_dpar(prep, \"mu\", i = i)\n  theta <- brms::get_dpar(prep, \"theta\", i = i)\n\n  hu <- runif(prep$ndraws, 0, 1)\n  ifelse(hu < theta, 2, rbinom(prep$ndraws, 1, pnorm(mu)))\n}\n```\n:::\n\n\nNow we can check our results using `pp_check()`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(fit_hurdle, ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nWe see that the model does an okay job recovering the 0's (no), 1's (yes), and 2's (abstain) from the original data.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}